# sample_workload_generator.py
# SKRueue ìƒ˜í”Œ ì›Œí¬ë¡œë“œ ìƒì„±ê¸°
# ë‹¤ì–‘í•œ ìœ í˜•ì˜ Spark + Iceberg ë°°ì¹˜ ì‘ì—…ì„ ìë™ìœ¼ë¡œ ìƒì„±í•˜ì—¬ ì‹¤í—˜ í™˜ê²½ êµ¬ì¶•
# Spark Operator ë¬¸ì œ ëŒ€ì‘ ë° ì•ˆì •ì„± ê°œì„  ë²„ì „

import os
import time
import yaml
import random
import logging
import argparse
import subprocess
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from dataclasses import dataclass
import json

@dataclass
class WorkloadTemplate:
    """ì›Œí¬ë¡œë“œ í…œí”Œë¦¿"""
    name: str
    category: str  # 'cpu-intensive', 'memory-intensive', 'io-intensive', 'mixed'
    description: str
    spark_config: Dict
    estimated_duration_minutes: int
    resource_requirements: Dict
    priority: int
    failure_rate: float = 0.0  # ì˜ë„ì  ì‹¤íŒ¨ìœ¨ (í…ŒìŠ¤íŠ¸ìš©)

class SparkJobGenerator:
    """Spark ì‘ì—… ìƒì„±ê¸° - ì•ˆì •ì„± ê°œì„  ë²„ì „"""
    
    def __init__(self, namespace: str = "skrueue-test", 
                 image_repository: str = "apache/spark:3.4.0",
                 fallback_to_k8s_jobs: bool = True):
        self.namespace = namespace
        self.image_repository = image_repository
        self.fallback_to_k8s_jobs = fallback_to_k8s_jobs
        self.logger = logging.getLogger('SparkJobGenerator')
        self.job_counter = 0
        
        # í™˜ê²½ ìƒíƒœ í™•ì¸
        self.spark_operator_available = self._check_spark_operator()
        self.service_account_ready = self._check_service_account()
        
        # ì›Œí¬ë¡œë“œ í…œí”Œë¦¿ ì •ì˜
        self.templates = self._define_templates()
        
        # ìƒíƒœ ë¡œê¹…
        self.logger.info(f"í™˜ê²½ ìƒíƒœ - Spark Operator: {self.spark_operator_available}, ServiceAccount: {self.service_account_ready}")
        
    def _check_spark_operator(self) -> bool:
        """Spark Operator ìƒíƒœ í™•ì¸"""
        try:
            result = subprocess.run(['kubectl', 'get', 'pods', '-n', 'spark-operator'], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                # ì›¹í›… ìƒíƒœ í™•ì¸
                webhook_result = subprocess.run(
                    ['kubectl', 'get', 'pods', '-n', 'spark-operator', '-l', 'app.kubernetes.io/name=spark-operator-webhook'],
                    capture_output=True, text=True
                )
                if 'CrashLoopBackOff' in webhook_result.stdout:
                    self.logger.warning("Spark Operator ì›¹í›…ì´ CrashLoopBackOff ìƒíƒœì…ë‹ˆë‹¤.")
                    return False
                return True
            return False
        except Exception as e:
            self.logger.warning(f"Spark Operator ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False
    
    def _check_service_account(self) -> bool:
        """ServiceAccount ì¡´ì¬ í™•ì¸"""
        try:
            result = subprocess.run([
                'kubectl', 'get', 'serviceaccount', 'skrueue-agent', '-n', self.namespace
            ], capture_output=True, text=True)
            return result.returncode == 0
        except Exception as e:
            self.logger.warning(f"ServiceAccount í™•ì¸ ì‹¤íŒ¨: {e}")
            return False
    
    def _setup_service_account(self):
        """ServiceAccount ë° ê¶Œí•œ ì„¤ì •"""
        try:
            self.logger.info("ServiceAccount ë° ê¶Œí•œ ì„¤ì • ì¤‘...")
            
            # ServiceAccount ìƒì„±
            sa_yaml = {
                'apiVersion': 'v1',
                'kind': 'ServiceAccount',
                'metadata': {
                    'name': 'skrueue-agent',
                    'namespace': self.namespace
                }
            }
            
            # Role ìƒì„±
            role_yaml = {
                'apiVersion': 'rbac.authorization.k8s.io/v1',
                'kind': 'Role',
                'metadata': {
                    'namespace': self.namespace,
                    'name': 'spark-role'
                },
                'rules': [
                    {
                        'apiGroups': [''],
                        'resources': ['pods', 'services', 'configmaps'],
                        'verbs': ['get', 'list', 'watch', 'create', 'update', 'patch', 'delete']
                    },
                    {
                        'apiGroups': [''],
                        'resources': ['secrets'],
                        'verbs': ['get', 'list', 'watch']
                    }
                ]
            }
            
            # RoleBinding ìƒì„±
            binding_yaml = {
                'apiVersion': 'rbac.authorization.k8s.io/v1',
                'kind': 'RoleBinding',
                'metadata': {
                    'name': 'spark-rolebinding',
                    'namespace': self.namespace
                },
                'subjects': [{
                    'kind': 'ServiceAccount',
                    'name': 'skrueue-agent',
                    'namespace': self.namespace
                }],
                'roleRef': {
                    'kind': 'Role',
                    'name': 'spark-role',
                    'apiGroup': 'rbac.authorization.k8s.io'
                }
            }
            
            # ë¦¬ì†ŒìŠ¤ë“¤ ì ìš©
            for yaml_content in [sa_yaml, role_yaml, binding_yaml]:
                self._apply_yaml(yaml_content)
            
            self.service_account_ready = True
            self.logger.info("ServiceAccount ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ServiceAccount ì„¤ì • ì‹¤íŒ¨: {e}")
            
    def _apply_yaml(self, yaml_content: dict):
        """YAML ë¦¬ì†ŒìŠ¤ ì ìš©"""
        try:
            process = subprocess.Popen(
                ['kubectl', 'apply', '-f', '-'],
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            stdout, stderr = process.communicate(input=yaml.dump(yaml_content))
            
            if process.returncode != 0:
                self.logger.warning(f"YAML ì ìš© ê²½ê³ : {stderr}")
                
        except Exception as e:
            self.logger.error(f"YAML ì ìš© ì‹¤íŒ¨: {e}")
        
    def _define_templates(self) -> List[WorkloadTemplate]:
        """ë‹¤ì–‘í•œ ì›Œí¬ë¡œë“œ í…œí”Œë¦¿ ì •ì˜"""
        return [
            # 1. CPU ì§‘ì•½ì  ì‘ì—… (ë¨¸ì‹ ëŸ¬ë‹ í›ˆë ¨)
            WorkloadTemplate(
                name="ml-training",
                category="cpu-intensive",
                description="Machine Learning Training with Spark MLlib",
                spark_config={
                    "spark.executor.instances": "3",
                    "spark.executor.cores": "4", 
                    "spark.executor.memory": "4g",
                    "spark.driver.memory": "2g",
                    "spark.sql.adaptive.enabled": "true",
                    "spark.sql.adaptive.coalescePartitions.enabled": "true"
                },
                estimated_duration_minutes=45,
                resource_requirements={
                    "cpu": "4000m",
                    "memory": "6Gi"
                },
                priority=8
            ),
            
            # 2. ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì‘ì—… (ëŒ€ìš©ëŸ‰ ë°ì´í„° ì§‘ê³„)
            WorkloadTemplate(
                name="big-data-aggregation",
                category="memory-intensive", 
                description="Large Dataset Aggregation with Window Functions",
                spark_config={
                    "spark.executor.instances": "4",
                    "spark.executor.cores": "2",
                    "spark.executor.memory": "8g",
                    "spark.driver.memory": "4g",
                    "spark.sql.shuffle.partitions": "400",
                    "spark.executor.memoryFraction": "0.8"
                },
                estimated_duration_minutes=60,
                resource_requirements={
                    "cpu": "2000m", 
                    "memory": "12Gi"
                },
                priority=6
            ),
            
            # 3. I/O ì§‘ì•½ì  ì‘ì—… (ETL íŒŒì´í”„ë¼ì¸)
            WorkloadTemplate(
                name="etl-pipeline",
                category="io-intensive",
                description="ETL Pipeline with Multiple Data Sources",
                spark_config={
                    "spark.executor.instances": "5",
                    "spark.executor.cores": "2",
                    "spark.executor.memory": "6g", 
                    "spark.driver.memory": "3g",
                    "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
                    "spark.sql.adaptive.skewJoin.enabled": "true"
                },
                estimated_duration_minutes=35,
                resource_requirements={
                    "cpu": "2000m",
                    "memory": "8Gi"
                },
                priority=5
            ),
            
            # 4. í˜¼í•© ì›Œí¬ë¡œë“œ (ì‹¤ì‹œê°„ ë¶„ì„)
            WorkloadTemplate(
                name="realtime-analytics",
                category="mixed",
                description="Real-time Analytics with Complex Joins",
                spark_config={
                    "spark.executor.instances": "3",
                    "spark.executor.cores": "3",
                    "spark.executor.memory": "5g",
                    "spark.driver.memory": "2g",
                    "spark.sql.adaptive.enabled": "true",
                    "spark.sql.adaptive.localShuffleReader.enabled": "true"
                },
                estimated_duration_minutes=25,
                resource_requirements={
                    "cpu": "3000m",
                    "memory": "7Gi"
                },
                priority=7
            ),
            
            # 5. ê²½ëŸ‰ ì‘ì—… (ë°ì´í„° ê²€ì¦)
            WorkloadTemplate(
                name="data-validation",
                category="lightweight",
                description="Data Quality Validation and Profiling",
                spark_config={
                    "spark.executor.instances": "2",
                    "spark.executor.cores": "1",
                    "spark.executor.memory": "2g",
                    "spark.driver.memory": "1g"
                },
                estimated_duration_minutes=15,
                resource_requirements={
                    "cpu": "1000m",
                    "memory": "3Gi"
                },
                priority=3
            ),
            
            # 6. ë©”ëª¨ë¦¬ ë¶€ì¡± ìœ ë°œ ì‘ì—… (í…ŒìŠ¤íŠ¸ìš©)
            WorkloadTemplate(
                name="memory-stress-test",
                category="memory-intensive",
                description="Memory Stress Test for OOM Simulation",
                spark_config={
                    "spark.executor.instances": "2",
                    "spark.executor.cores": "2",
                    "spark.executor.memory": "6g",  # ì‹¤ì œë¡œëŠ” ë” ë§ì´ ì‚¬ìš©
                    "spark.driver.memory": "2g",
                    "spark.sql.shuffle.partitions": "50"  # ì ì€ íŒŒí‹°ì…˜ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì••ë°•
                },
                estimated_duration_minutes=30,
                resource_requirements={
                    "cpu": "2000m",
                    "memory": "8Gi"  # ìš”ì²­ë³´ë‹¤ ì‹¤ì œ ì‚¬ìš©ëŸ‰ì´ ë§ìŒ
                },
                priority=4,
                failure_rate=0.3  # 30% ì‹¤íŒ¨ìœ¨
            )
        ]
        
    def generate_spark_application_yaml(self, template: WorkloadTemplate, 
                                      additional_config: Dict = None) -> str:
        """SparkApplication YAML ìƒì„±"""
        self.job_counter += 1
        job_name = f"{template.name}-{self.job_counter:04d}"
        
        # ì¶”ê°€ ì„¤ì • ë³‘í•©
        spark_config = template.spark_config.copy()
        if additional_config:
            spark_config.update(additional_config)
            
        # Python ì½”ë“œ ìƒì„± (í…œí”Œë¦¿ë³„ë¡œ ë‹¤ë¥¸ ì‘ì—… ìˆ˜í–‰)
        python_code = self._generate_python_code(template)
        
        spark_app = {
            'apiVersion': 'sparkoperator.k8s.io/v1beta2',
            'kind': 'SparkApplication',
            'metadata': {
                'name': job_name,
                'namespace': self.namespace,
                'labels': {
                    'app': 'skrueue-test',
                    'workload-type': template.category,
                    'scheduler': 'skrueue'
                },
                'annotations': {
                    'skrueue.ai/estimated-duration': str(template.estimated_duration_minutes),
                    'skrueue.ai/workload-category': template.category,
                    'skrueue.ai/description': template.description
                }
            },
            'spec': {
                'sparkVersion': self.image_repository.split(':')[-1],
                'type': 'Python',
                'pythonVersion': '3',
                'mode': 'cluster',
                'image': self.image_repository,
                'imagePullPolicy': 'IfNotPresent',
                'mainApplicationFile': 'local:///tmp/spark-job.py',
                'sparkConf': spark_config,
                'driver': {
                    'cores': 1,
                    'memory': spark_config.get('spark.driver.memory', '2g'),
                    'serviceAccount': 'skrueue-agent',
                    'labels': {
                        'version': '3.4.0',
                        'job-name': job_name
                    }
                },
                'executor': {
                    'cores': int(spark_config.get('spark.executor.cores', '2')),
                    'instances': int(spark_config.get('spark.executor.instances', '2')),
                    'memory': spark_config.get('spark.executor.memory', '4g'),
                    'labels': {
                        'version': '3.4.0',
                        'job-name': job_name
                    }
                },
                'restartPolicy': {
                    'type': 'Never'
                }
            }
        }
        
        # ConfigMapìœ¼ë¡œ Python ì½”ë“œ ì£¼ì…
        configmap = {
            'apiVersion': 'v1',
            'kind': 'ConfigMap',
            'metadata': {
                'name': f"{job_name}-config",
                'namespace': self.namespace
            },
            'data': {
                'spark-job.py': python_code
            }
        }
        
        # Volume Mount ì¶”ê°€
        spark_app['spec']['driver']['volumeMounts'] = [{
            'name': 'job-config',
            'mountPath': '/tmp'
        }]
        spark_app['spec']['executor']['volumeMounts'] = [{
            'name': 'job-config', 
            'mountPath': '/tmp'
        }]
        
        spark_app['spec']['volumes'] = [{
            'name': 'job-config',
            'configMap': {
                'name': f"{job_name}-config"
            }
        }]
        
        # ë‘ ê°œì˜ YAML ë¬¸ì„œë¥¼ ê²°í•©
        combined_yaml = yaml.dump(configmap) + "\n---\n" + yaml.dump(spark_app)
        
        return combined_yaml

    def generate_k8s_job_yaml(self, template: WorkloadTemplate) -> str:
        """ëŒ€ì•ˆìœ¼ë¡œ Kubernetes Job YAML ìƒì„±"""
        self.job_counter += 1
        job_name = f"k8s-{template.name}-{self.job_counter:04d}"
        
        # ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­ ë§¤í•‘
        resource_map = {
            "cpu-intensive": {"cpu": "1000m", "memory": "2Gi"},
            "memory-intensive": {"cpu": "500m", "memory": "4Gi"},
            "io-intensive": {"cpu": "800m", "memory": "1Gi"},
            "mixed": {"cpu": "800m", "memory": "2Gi"},
            "lightweight": {"cpu": "200m", "memory": "512Mi"}
        }
        
        resources = resource_map.get(template.category, {"cpu": "500m", "memory": "1Gi"})
        
        # ì‘ì—… ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
        script = self._generate_k8s_job_script(template)
        
        job_yaml = {
            'apiVersion': 'batch/v1',
            'kind': 'Job',
            'metadata': {
                'name': job_name,
                'namespace': self.namespace,
                'labels': {
                    'app': 'skrueue-test',
                    'workload-type': template.category,
                    'scheduler': 'default-scheduler',
                    'priority': str(template.priority),
                    'template': template.name
                },
                'annotations': {
                    'skrueue.ai/estimated-duration': str(template.estimated_duration_minutes),
                    'skrueue.ai/workload-category': template.category,
                    'skrueue.ai/description': template.description,
                    'skrueue.ai/fallback-job': 'true'
                }
            },
            'spec': {
                'backoffLimit': 2,
                'activeDeadlineSeconds': template.estimated_duration_minutes * 60 * 2,
                'template': {
                    'metadata': {
                        'labels': {
                            'job-name': job_name,
                            'workload-type': template.category
                        }
                    },
                    'spec': {
                        'restartPolicy': 'Never',
                        'containers': [{
                            'name': 'worker',
                            'image': 'busybox:1.35',
                            'command': ['sh', '-c', script],
                            'resources': {
                                'requests': resources,
                                'limits': {
                                    'cpu': str(int(resources['cpu'][:-1]) * 2) + 'm',
                                    'memory': resources['memory']
                                }
                            },
                            'env': [
                                {'name': 'JOB_NAME', 'value': job_name},
                                {'name': 'JOB_TYPE', 'value': template.category},
                                {'name': 'DURATION', 'value': str(template.estimated_duration_minutes * 60)},
                                {'name': 'TEMPLATE', 'value': template.name}
                            ]
                        }]
                    }
                }
            }
        }
        
        return yaml.dump(job_yaml)

    def _generate_k8s_job_script(self, template: WorkloadTemplate) -> str:
        """Kubernetes Jobìš© ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        duration = template.estimated_duration_minutes * 60
        
        base_script = f'''echo "ğŸš€ {template.description}"
echo "ì¹´í…Œê³ ë¦¬: {template.category}"
echo "ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„: {template.estimated_duration_minutes}ë¶„"
echo "ì‹œì‘ ì‹œê°„: $(date)"

start_time=$(date +%s)
iteration=0
'''

        if template.category == "cpu-intensive":
            work_script = '''
# CPU ì§‘ì•½ì  ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
while [ $(($(date +%s) - start_time)) -lt ''' + str(duration) + ''' ]; do
    for i in $(seq 1 1000); do
        result=$(awk 'BEGIN {for(i=1;i<=100;i++) sum+=sqrt(i*i); print sum}')
    done
    iteration=$((iteration + 1))
    if [ $((iteration % 10)) -eq 0 ]; then
        elapsed=$(($(date +%s) - start_time))
        echo "CPU ì‘ì—… ì§„í–‰ë¥ : $((elapsed * 100 / ''' + str(duration) + '''))% (ë°˜ë³µ: $iteration)"
    fi
done'''

        elif template.category == "memory-intensive":
            work_script = '''
# ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
while [ $(($(date +%s) - start_time)) -lt ''' + str(duration) + ''' ]; do
    temp_file="/tmp/mem_test_$(date +%s%N)"
    dd if=/dev/zero of="$temp_file" bs=1M count=50 2>/dev/null
    iteration=$((iteration + 1))
    if [ $((iteration % 5)) -eq 0 ]; then
        elapsed=$(($(date +%s) - start_time))
        echo "ë©”ëª¨ë¦¬ ì‘ì—… ì§„í–‰ë¥ : $((elapsed * 100 / ''' + str(duration) + '''))% (íŒŒì¼: $iterationê°œ)"
        ls /tmp/mem_test_* 2>/dev/null | wc -l
    fi
    sleep 3
done
rm -f /tmp/mem_test_* 2>/dev/null'''

        elif template.category == "io-intensive":
            work_script = '''
# I/O ì§‘ì•½ì  ì‘ì—… ì‹œë®¬ë ˆì´ì…˜  
while [ $(($(date +%s) - start_time)) -lt ''' + str(duration) + ''' ]; do
    test_file="/tmp/io_test_$iteration"
    for i in $(seq 1 50); do
        echo "ë°ì´í„° ë¼ì¸ $i: $(date +%s%N)" >> "$test_file"
    done
    wc -l "$test_file" >/dev/null
    rm -f "$test_file"
    iteration=$((iteration + 1))
    if [ $((iteration % 20)) -eq 0 ]; then
        elapsed=$(($(date +%s) - start_time))
        echo "I/O ì‘ì—… ì§„í–‰ë¥ : $((elapsed * 100 / ''' + str(duration) + '''))% (íŒŒì¼ ì²˜ë¦¬: $iterationê°œ)"
    fi
    sleep 1
done'''

        else:  # mixed, lightweight
            work_script = '''
# í˜¼í•©/ê²½ëŸ‰ ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
while [ $(($(date +%s) - start_time)) -lt ''' + str(duration) + ''' ]; do
    # ê°„ë‹¨í•œ ê³„ì‚°
    result=$(awk 'BEGIN {print sqrt(''' + str(random.randint(1, 1000)) + ''')}')
    
    # ê°„ë‹¨í•œ íŒŒì¼ ì‘ì—…
    echo "ì‘ì—… $iteration: $result" > "/tmp/work_$iteration"
    cat "/tmp/work_$iteration" >/dev/null
    rm -f "/tmp/work_$iteration"
    
    iteration=$((iteration + 1))
    if [ $((iteration % 30)) -eq 0 ]; then
        elapsed=$(($(date +%s) - start_time))
        echo "ì‘ì—… ì§„í–‰ë¥ : $((elapsed * 100 / ''' + str(duration) + '''))% (ì²˜ë¦¬: $iterationíšŒ)"
    fi
    sleep 2
done'''

        end_script = '''
elapsed=$(($(date +%s) - start_time))
echo "âœ… ì‘ì—… ì™„ë£Œ: $(date)"
echo "ì‹¤ì œ ì‹¤í–‰ ì‹œê°„: ${elapsed}ì´ˆ"
echo "ì´ ë°˜ë³µ íšŸìˆ˜: $iteration"
'''

        return base_script + work_script + end_script
        
    def _generate_python_code(self, template: WorkloadTemplate) -> str:
        """í…œí”Œë¦¿ë³„ Python ì½”ë“œ ìƒì„± (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)"""
        
        base_imports = f'''
import time
import random
import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

spark = SparkSession.builder.appName("{template.name}").getOrCreate()
print(f"Starting job: {template.description}")
start_time = time.time()

try:
'''

        # í…œí”Œë¦¿ë³„ íŠ¹í™” ì½”ë“œ (ê¸°ì¡´ê³¼ ë™ì¼)
        if template.category == "cpu-intensive":
            job_code = '''
    # CPU ì§‘ì•½ì  ì‘ì—…: ë³µì¡í•œ ìˆ˜í•™ ì—°ì‚°
    data = [(i, random.random(), random.random()) for i in range(1000000)]
    df = spark.createDataFrame(data, ["id", "x", "y"])
    
    # ë³µì¡í•œ ìˆ˜í•™ ì—°ì‚° ìˆ˜í–‰
    result = df.select(
        col("id"),
        sin(col("x")).alias("sin_x"),
        cos(col("y")).alias("cos_y"),
        sqrt(col("x") * col("x") + col("y") * col("y")).alias("distance"),
        when(col("x") > 0.5, exp(col("x"))).otherwise(log(col("x") + 1)).alias("complex_calc")
    ).groupBy((col("id") % 100).alias("bucket")).agg(
        avg("sin_x").alias("avg_sin"),
        max("cos_y").alias("max_cos"),
        sum("distance").alias("total_distance"),
        count("*").alias("count")
    )
    
    print(f"CPU ì§‘ì•½ì  ì‘ì—… ì™„ë£Œ. ê²°ê³¼ ë ˆì½”ë“œ ìˆ˜: {result.count()}")
'''

        elif template.category == "memory-intensive":
            job_code = '''
    # ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì‘ì—…: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
    # í° ë°ì´í„°ì…‹ ìƒì„±
    large_data = [(i, f"user_{i}", random.randint(1, 1000), random.random() * 10000) 
                  for i in range(5000000)]  # 5M ë ˆì½”ë“œ
    df1 = spark.createDataFrame(large_data, ["id", "user", "category", "amount"])
    
    # ë©”ëª¨ë¦¬ ì§‘ì•½ì  ìœˆë„ìš° í•¨ìˆ˜
    from pyspark.sql.window import Window
    window_spec = Window.partitionBy("category").orderBy("amount")
    
    result = df1.withColumn("rank", row_number().over(window_spec)) \\
               .withColumn("cumsum", sum("amount").over(window_spec)) \\
               .cache()  # ë©”ëª¨ë¦¬ì— ìºì‹œ
    
    # ì—¬ëŸ¬ ë²ˆ ì ‘ê·¼í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€
    for i in range(3):
        count = result.filter(col("rank") <= 100).count()
        print(f"Iteration {i+1}: Top 100 per category count: {count}")
        time.sleep(10)  # ë©”ëª¨ë¦¬ ì••ë°• ì§€ì†
'''

        elif template.category == "io-intensive":
            job_code = '''
    # I/O ì§‘ì•½ì  ì‘ì—…: ë‹¤ì¤‘ ì¡°ì¸ ë° íŒŒì¼ ì²˜ë¦¬
    # ì—¬ëŸ¬ ë°ì´í„°ì…‹ ìƒì„±
    users = [(i, f"user_{i}", random.choice(["A", "B", "C"])) for i in range(100000)]
    orders = [(i, random.randint(1, 100000), random.random() * 1000) for i in range(500000)]
    products = [(i, f"product_{i}", random.random() * 100) for i in range(10000)]
    
    users_df = spark.createDataFrame(users, ["user_id", "user_name", "segment"])
    orders_df = spark.createDataFrame(orders, ["order_id", "user_id", "amount"])
    products_df = spark.createDataFrame(products, ["product_id", "product_name", "price"])
    
    # ë³µì¡í•œ ì¡°ì¸ ì—°ì‚°
    result = orders_df.join(users_df, "user_id") \\
                     .join(products_df, orders_df.order_id % 10000 == products_df.product_id) \\
                     .groupBy("segment", "product_name") \\
                     .agg(
                         sum("amount").alias("total_sales"),
                         count("*").alias("order_count"),
                         avg("price").alias("avg_price")
                     )
    
    print(f"I/O ì§‘ì•½ì  ì‘ì—… ì™„ë£Œ. ê²°ê³¼ ë ˆì½”ë“œ ìˆ˜: {result.count()}")
'''

        elif template.category == "mixed":
            job_code = '''
    # í˜¼í•© ì›Œí¬ë¡œë“œ: CPU + ë©”ëª¨ë¦¬ + I/O
    # ì‹œê³„ì—´ ë°ì´í„° ìƒì„±
    import datetime
    
    time_series_data = []
    for i in range(1000000):
        timestamp = datetime.datetime.now() - datetime.timedelta(seconds=i)
        time_series_data.append((
            timestamp,
            random.choice(["sensor_1", "sensor_2", "sensor_3"]),
            random.random() * 100,
            random.randint(0, 1)
        ))
    
    df = spark.createDataFrame(time_series_data, 
                              ["timestamp", "sensor_id", "value", "status"])
    
    # ë³µì¡í•œ ì‹œê³„ì—´ ë¶„ì„
    result = df.withColumn("hour", hour("timestamp")) \\
               .groupBy("sensor_id", "hour") \\
               .agg(
                   avg("value").alias("avg_value"),
                   stddev("value").alias("stddev_value"),
                   max("value").alias("max_value"),
                   sum(when(col("status") == 1, 1).otherwise(0)).alias("active_count")
               ) \\
               .withColumn("anomaly_score", 
                          when(col("stddev_value") > 20, 1).otherwise(0))
    
    print(f"í˜¼í•© ì›Œí¬ë¡œë“œ ì™„ë£Œ. ê²°ê³¼ ë ˆì½”ë“œ ìˆ˜: {result.count()}")
'''

        elif template.category == "lightweight":
            job_code = '''
    # ê²½ëŸ‰ ì‘ì—…: ê°„ë‹¨í•œ ë°ì´í„° ê²€ì¦
    sample_data = [(i, f"value_{i}", random.choice([None, "valid", "invalid"])) 
                   for i in range(10000)]
    df = spark.createDataFrame(sample_data, ["id", "value", "status"])
    
    # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
    total_count = df.count()
    null_count = df.filter(col("status").isNull()).count()
    valid_count = df.filter(col("status") == "valid").count()
    
    print(f"ë°ì´í„° ê²€ì¦ ì™„ë£Œ - ì „ì²´: {total_count}, NULL: {null_count}, ìœ íš¨: {valid_count}")
'''

        else:  # memory-stress-test
            job_code = f'''
    # ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸: ì˜ë„ì ìœ¼ë¡œ OOM ìœ ë°œ
    print("ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # ì ì§„ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€
    data_sizes = [1000000, 2000000, 5000000]  # ì ì  í° ë°ì´í„°ì…‹
    
    for size in data_sizes:
        print(f"Processing dataset of size: {{size}}")
        large_data = [(i, f"data_{{i}}" * 100, random.random()) for i in range(size)]
        df = spark.createDataFrame(large_data, ["id", "large_text", "value"])
        
        # ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì—°ì‚°
        df.cache()
        result = df.groupBy((col("id") % 10).alias("group")).agg(
            collect_list("large_text").alias("text_list"),  # ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©
            sum("value").alias("total_value")
        )
        
        count = result.count()
        print(f"Processed {{count}} groups for size {{size}}")
        
        # ì‹¤íŒ¨ í™•ë¥  ì ìš©
        if random.random() < {template.failure_rate}:
            print("Simulating failure...")
            raise Exception("Simulated OOM or processing failure")
            
        time.sleep(15)  # ë©”ëª¨ë¦¬ ì••ë°• ì§€ì†
'''

        cleanup_code = '''
except Exception as e:
    print(f"Job failed with error: {e}")
    sys.exit(1)
finally:
    elapsed_time = time.time() - start_time
    print(f"Job completed in {elapsed_time:.2f} seconds")
    spark.stop()
'''

        return base_imports + job_code + cleanup_code

    def submit_job(self, template: WorkloadTemplate, 
                   additional_config: Dict = None, force_k8s_job: bool = False) -> bool:
        """ì‘ì—…ì„ í´ëŸ¬ìŠ¤í„°ì— ì œì¶œ - ì•ˆì •ì„± ê°œì„ """
        
        # ServiceAccount í™•ì¸ ë° ì„¤ì •
        if not self.service_account_ready:
            self._setup_service_account()
        
        # SparkApplication ë˜ëŠ” Kubernetes Job ê²°ì •
        use_spark = (self.spark_operator_available and 
                    self.service_account_ready and 
                    not force_k8s_job)
        
        try:
            if use_spark:
                yaml_content = self.generate_spark_application_yaml(template, additional_config)
                job_type = "SparkApplication"
            else:
                yaml_content = self.generate_k8s_job_yaml(template)
                job_type = "Kubernetes Job"
                
            self.logger.info(f"ì œì¶œí•  ì‘ì—… ìœ í˜•: {job_type}")
            
            # kubectl applyë¡œ ì œì¶œ
            process = subprocess.Popen(
                ['kubectl', 'apply', '-f', '-'],
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE,
                text=True
            )
            
            stdout, stderr = process.communicate(input=yaml_content)
            
            if process.returncode == 0:
                if use_spark:
                    job_name = f"{template.name}-{self.job_counter:04d}"
                else:
                    job_name = f"k8s-{template.name}-{self.job_counter:04d}"
                    
                self.logger.info(f"ì‘ì—… ì œì¶œ ì„±ê³µ: {job_name} ({job_type})")
                
                # ì‘ì—… ìƒíƒœ í™•ì¸ (5ì´ˆ í›„)
                time.sleep(5)
                self._verify_job_status(job_name, use_spark)
                
                return True
            else:
                self.logger.error(f"ì‘ì—… ì œì¶œ ì‹¤íŒ¨: {stderr}")
                
                # SparkApplication ì‹¤íŒ¨ ì‹œ Kubernetes Jobìœ¼ë¡œ ì¬ì‹œë„
                if use_spark and self.fallback_to_k8s_jobs:
                    self.logger.info("SparkApplication ì‹¤íŒ¨, Kubernetes Jobìœ¼ë¡œ ì¬ì‹œë„...")
                    return self.submit_job(template, additional_config, force_k8s_job=True)
                    
                return False
                
        except Exception as e:
            self.logger.error(f"ì‘ì—… ì œì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ì˜ˆì™¸ ë°œìƒ ì‹œë„ Kubernetes Jobìœ¼ë¡œ ì¬ì‹œë„
            if use_spark and self.fallback_to_k8s_jobs:
                self.logger.info("ì˜ˆì™¸ ë°œìƒ, Kubernetes Jobìœ¼ë¡œ ì¬ì‹œë„...")
                return self.submit_job(template, additional_config, force_k8s_job=True)
                
            return False

    def _verify_job_status(self, job_name: str, is_spark_app: bool):
        """ì‘ì—… ìƒíƒœ í™•ì¸"""
        try:
            if is_spark_app:
                result = subprocess.run([
                    'kubectl', 'get', 'sparkapplication', job_name, '-n', self.namespace
                ], capture_output=True, text=True)
            else:
                result = subprocess.run([
                    'kubectl', 'get', 'job', job_name, '-n', self.namespace
                ], capture_output=True, text=True)
                
            if result.returncode == 0:
                self.logger.info(f"ì‘ì—… ìƒíƒœ í™•ì¸ ì„±ê³µ: {job_name}")
            else:
                self.logger.warning(f"ì‘ì—… ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {job_name}")
                
        except Exception as e:
            self.logger.warning(f"ì‘ì—… ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")

class WorkloadGenerator:
    """ì›Œí¬ë¡œë“œ ìƒì„± ë©”ì¸ í´ë˜ìŠ¤ - ì•ˆì •ì„± ê°œì„  ë²„ì „"""
    
    def __init__(self, namespace: str = "skrueue-test"):
        self.namespace = namespace
        self.logger = logging.getLogger('WorkloadGenerator')
        self.spark_generator = SparkJobGenerator(namespace)
        
    def generate_realistic_workload_pattern(self, duration_hours: int = 24) -> List[Dict]:
        """í˜„ì‹¤ì ì¸ ì›Œí¬ë¡œë“œ íŒ¨í„´ ìƒì„± (ê¸°ì¡´ê³¼ ë™ì¼)"""
        workload_schedule = []
        
        # ì‹œê°„ëŒ€ë³„ ì›Œí¬ë¡œë“œ íŒ¨í„´ ì •ì˜
        hourly_patterns = {
            # ìƒˆë²½ ì‹œê°„ (0-6ì‹œ): ë°°ì¹˜ ì‘ì—… ì¤‘ì‹¬
            range(0, 6): {
                'job_rate': 2,  # ì‹œê°„ë‹¹ ì‘ì—… ìˆ˜
                'template_weights': {
                    'etl-pipeline': 0.4,
                    'big-data-aggregation': 0.3,
                    'data-validation': 0.2,
                    'memory-stress-test': 0.1
                }
            },
            # ì—…ë¬´ ì‹œê°„ (9-18ì‹œ): ë‹¤ì–‘í•œ ë¶„ì„ ì‘ì—…
            range(9, 18): {
                'job_rate': 5,
                'template_weights': {
                    'realtime-analytics': 0.3,
                    'ml-training': 0.25,
                    'etl-pipeline': 0.2,
                    'big-data-aggregation': 0.15,
                    'data-validation': 0.1
                }
            },
            # ì €ë… ì‹œê°„ (19-23ì‹œ): ë³´ê³ ì„œ ìƒì„± ë° ë¶„ì„
            range(19, 24): {
                'job_rate': 3,
                'template_weights': {
                    'big-data-aggregation': 0.35,
                    'ml-training': 0.25,
                    'realtime-analytics': 0.2,
                    'etl-pipeline': 0.15,
                    'memory-stress-test': 0.05
                }
            }
        }
        
        # ê¸°ë³¸ íŒ¨í„´ (ë‹¤ë¥¸ ì‹œê°„ëŒ€)
        default_pattern = {
            'job_rate': 1,
            'template_weights': {
                'data-validation': 0.4,
                'etl-pipeline': 0.3,
                'realtime-analytics': 0.2,
                'memory-stress-test': 0.1
            }
        }
        
        # ìŠ¤ì¼€ì¤„ ìƒì„±
        start_time = datetime.now()
        
        for hour in range(duration_hours):
            current_hour = (start_time.hour + hour) % 24
            
            # í•´ë‹¹ ì‹œê°„ëŒ€ íŒ¨í„´ ì°¾ê¸°
            pattern = default_pattern
            for hour_range, hour_pattern in hourly_patterns.items():
                if current_hour in hour_range:
                    pattern = hour_pattern
                    break
                    
            # í•´ë‹¹ ì‹œê°„ ë™ì•ˆ ì œì¶œí•  ì‘ì—…ë“¤ ìƒì„±
            num_jobs = max(1, int(random.gauss(pattern['job_rate'], 1)))
            
            for job_idx in range(num_jobs):
                # í…œí”Œë¦¿ ì„ íƒ (ê°€ì¤‘ì¹˜ ê¸°ë°˜)
                templates = list(pattern['template_weights'].keys())
                weights = list(pattern['template_weights'].values())
                selected_template = random.choices(templates, weights=weights)[0]
                
                # ì œì¶œ ì‹œê°„ (í•´ë‹¹ ì‹œê°„ ë‚´ ëœë¤)
                job_time = start_time + timedelta(
                    hours=hour,
                    minutes=random.randint(0, 59),
                    seconds=random.randint(0, 59)
                )
                
                workload_schedule.append({
                    'template_name': selected_template,
                    'submit_time': job_time,
                    'hour': current_hour
                })
                
        # ì‹œê°„ìˆœ ì •ë ¬
        workload_schedule.sort(key=lambda x: x['submit_time'])
        
        return workload_schedule
        
    def run_workload_simulation(self, schedule: List[Dict]):
        """ì›Œí¬ë¡œë“œ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ - ì•ˆì •ì„± ê°œì„ """
        self.logger.info(f"ì›Œí¬ë¡œë“œ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘: {len(schedule)}ê°œ ì‘ì—… ì˜ˆì •")
        
        templates_map = {t.name: t for t in self.spark_generator.templates}
        submitted_count = 0
        failed_count = 0
        
        for job_info in schedule:
            # ì œì¶œ ì‹œê°„ê¹Œì§€ ëŒ€ê¸°
            now = datetime.now()
            wait_time = (job_info['submit_time'] - now).total_seconds()
            
            if wait_time > 0:
                self.logger.info(f"ë‹¤ìŒ ì‘ì—…ê¹Œì§€ {wait_time:.1f}ì´ˆ ëŒ€ê¸°...")
                time.sleep(min(wait_time, 300))  # ìµœëŒ€ 5ë¶„ê¹Œì§€ë§Œ ëŒ€ê¸°
                
            # ì‘ì—… ì œì¶œ
            template_name = job_info['template_name']
            if template_name in templates_map:
                template = templates_map[template_name]
                
                # ì•½ê°„ì˜ ë³€ë™ì„± ì¶”ê°€
                additional_config = self._add_job_variance(template)
                
                success = self.spark_generator.submit_job(template, additional_config)
                if success:
                    submitted_count += 1
                else:
                    failed_count += 1
                    
                self.logger.info(f"ì§„í–‰ë¥ : {submitted_count + failed_count}/{len(schedule)} "
                               f"(ì„±ê³µ: {submitted_count}, ì‹¤íŒ¨: {failed_count})")
            else:
                self.logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” í…œí”Œë¦¿: {template_name}")
                failed_count += 1
                
        self.logger.info(f"ì›Œí¬ë¡œë“œ ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ: {submitted_count}ê°œ ì‘ì—… ì œì¶œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨")
        
    def _add_job_variance(self, template: WorkloadTemplate) -> Dict:
        """ì‘ì—…ì— ë³€ë™ì„± ì¶”ê°€ (ê¸°ì¡´ê³¼ ë™ì¼)"""
        variance_config = {}
        
        # ì‹¤í–‰ì ìˆ˜ ë³€ë™ (Â±1)
        base_instances = int(template.spark_config.get('spark.executor.instances', '2'))
        variance_config['spark.executor.instances'] = str(max(1, 
            base_instances + random.randint(-1, 1)))
            
        # ë©”ëª¨ë¦¬ ë³€ë™ (Â±20%)
        base_memory = template.spark_config.get('spark.executor.memory', '4g')
        memory_value = int(base_memory.replace('g', ''))
        variance_factor = random.uniform(0.8, 1.2)
        new_memory = max(1, int(memory_value * variance_factor))
        variance_config['spark.executor.memory'] = f"{new_memory}g"
        
        return variance_config

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='SKRueue ìƒ˜í”Œ ì›Œí¬ë¡œë“œ ìƒì„±ê¸° - ì•ˆì •ì„± ê°œì„  ë²„ì „')
    parser.add_argument('--mode', choices=['single', 'batch', 'simulation'], 
                       default='simulation', help='ì‹¤í–‰ ëª¨ë“œ')
    parser.add_argument('--template', type=str, help='ë‹¨ì¼ í…œí”Œë¦¿ ì´ë¦„')
    parser.add_argument('--count', type=int, default=10, help='ë°°ì¹˜ ëª¨ë“œ ì‘ì—… ìˆ˜')
    parser.add_argument('--interval', type=int, default=60, help='ë°°ì¹˜ ëª¨ë“œ ê°„ê²©(ì´ˆ)')
    parser.add_argument('--duration', type=int, default=24, help='ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„(ì‹œê°„)')
    parser.add_argument('--namespace', type=str, default='skrueue-test', 
                       help='ëŒ€ìƒ ë„¤ì„ìŠ¤í˜ì´ìŠ¤')
    parser.add_argument('--dry-run', action='store_true', help='ì‹¤ì œ ì œì¶œ ì—†ì´ YAMLë§Œ ì¶œë ¥')
    parser.add_argument('--force-k8s-jobs', action='store_true', 
                       help='SparkApplication ëŒ€ì‹  Kubernetes Job ê°•ì œ ì‚¬ìš©')
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    generator = WorkloadGenerator(args.namespace)
    
    # Kubernetes Job ê°•ì œ ì‚¬ìš© ì„¤ì •
    if args.force_k8s_jobs:
        generator.spark_generator.fallback_to_k8s_jobs = True
        generator.spark_generator.spark_operator_available = False
    
    if args.mode == 'single':
        # ë‹¨ì¼ ì‘ì—… ì œì¶œ
        if not args.template:
            print("ì‚¬ìš© ê°€ëŠ¥í•œ í…œí”Œë¦¿:")
            for template in generator.spark_generator.templates:
                print(f"  - {template.name}: {template.description}")
            return
            
        template = next((t for t in generator.spark_generator.templates 
                        if t.name == args.template), None)
        if not template:
            print(f"í…œí”Œë¦¿ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {args.template}")
            return
            
        if args.dry_run:
            if args.force_k8s_jobs:
                yaml_content = generator.spark_generator.generate_k8s_job_yaml(template)
            else:
                yaml_content = generator.spark_generator.generate_spark_application_yaml(template)
            print(yaml_content)
        else:
            success = generator.spark_generator.submit_job(template, force_k8s_job=args.force_k8s_jobs)
            print(f"ì‘ì—… ì œì¶œ {'ì„±ê³µ' if success else 'ì‹¤íŒ¨'}")
            
    elif args.mode == 'batch':
        # ë°°ì¹˜ ëª¨ë“œ: ëœë¤ ì‘ì—…ë“¤ì„ ì¼ì • ê°„ê²©ìœ¼ë¡œ ì œì¶œ
        success_count = 0
        for i in range(args.count):
            template = random.choice(generator.spark_generator.templates)
            print(f"[{i+1}/{args.count}] ì œì¶œ ì¤‘: {template.name}")
            
            if not args.dry_run:
                if generator.spark_generator.submit_job(template, force_k8s_job=args.force_k8s_jobs):
                    success_count += 1
                    
            if i < args.count - 1:  # ë§ˆì§€ë§‰ ì‘ì—…ì´ ì•„ë‹ˆë©´ ëŒ€ê¸°
                time.sleep(args.interval)
                
        if not args.dry_run:
            print(f"ë°°ì¹˜ ì‘ì—… ì™„ë£Œ: {success_count}/{args.count} ì„±ê³µ")
                
    elif args.mode == 'simulation':
        # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ: í˜„ì‹¤ì ì¸ ì›Œí¬ë¡œë“œ íŒ¨í„´
        schedule = generator.generate_realistic_workload_pattern(args.duration)
        
        if args.dry_run:
            print(f"ìƒì„±ëœ ìŠ¤ì¼€ì¤„ ({len(schedule)}ê°œ ì‘ì—…):")
            for job_info in schedule[:10]:  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
                print(f"  {job_info['submit_time']}: {job_info['template_name']}")
            print("...")
        else:
            generator.run_workload_simulation(schedule)

if __name__ == "__main__":
    main()