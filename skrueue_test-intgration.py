# skrueue_integration_test.py
# SKRueue í†µí•© í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ - ì‹¤ì œ RL ëª¨ë¸ í›ˆë ¨ ë° ì„±ëŠ¥ ë¹„êµ
# ì‹¤ì œ Kubernetes í´ëŸ¬ìŠ¤í„°ì—ì„œ RL ê¸°ë°˜ ìŠ¤ì¼€ì¤„ëŸ¬ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•˜ê³  í‰ê°€

import os
import time
import json
import logging
import threading
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, asdict
import yaml
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed

# SKRueue ëª¨ë“ˆ import
from skrueue import SKRueueEnvironment, RLAgent, KueueInterface, create_skrueue_environment

@dataclass
class TestConfiguration:
    """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
    test_duration_minutes: int = 30
    job_submission_interval: int = 20  # ì´ˆ
    max_concurrent_jobs: int = 8
    algorithms_to_test: List[str] = None
    baseline_scheduler: str = "kueue-default"
    output_dir: str = "skrueue_test_results"
    namespace: str = "skrueue-test"
    model_training_steps: int = 10000
    max_queue_size: int = 10
    
    def __post_init__(self):
        if self.algorithms_to_test is None:
            self.algorithms_to_test = ['DQN', 'PPO', 'A2C']

@dataclass
class JobTemplate:
    """í…ŒìŠ¤íŠ¸ ì‘ì—… í…œí”Œë¦¿"""
    name: str
    job_type: str
    cpu_request: str
    memory_request: str
    estimated_duration: int  # ë¶„
    image: str
    command: List[str]
    priority: int = 0

@dataclass
class TestResults:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    scheduler_name: str
    total_jobs_submitted: int
    total_jobs_completed: int
    total_jobs_failed: int
    average_wait_time: float  # ë¶„
    average_execution_time: float  # ë¶„
    resource_utilization: Dict[str, float]
    oom_incidents: int
    throughput: float  # jobs/hour
    success_rate: float  # %
    scheduler_decisions: List[Dict]

class JobGenerator:
    """í…ŒìŠ¤íŠ¸ìš© ì‘ì—… ìƒì„±ê¸° (ê²½ëŸ‰í™”)"""
    
    def __init__(self, namespace: str = "skrueue-test"):
        self.namespace = namespace
        self.logger = logging.getLogger('JobGenerator')
        self.job_counter = 0
        
        # ì‹¤í–‰ ê°€ëŠ¥í•œ ê°„ë‹¨í•œ ì‘ì—… í…œí”Œë¦¿ë“¤
        self.job_templates = [
            JobTemplate(
                name="cpu-task",
                job_type="cpu-intensive",
                cpu_request="1000m",
                memory_request="2Gi",
                estimated_duration=15,
                image="busybox:1.35",
                command=[
                    "sh", "-c",
                    "echo 'CPU task started'; "
                    "for i in $(seq 1 30); do "
                    "  awk 'BEGIN {for(i=1;i<=1000;i++) sum+=sqrt(i*i); print sum}' > /dev/null; "
                    "  sleep 30; "
                    "done; "
                    "echo 'CPU task completed'"
                ],
                priority=5
            ),
            JobTemplate(
                name="memory-task",
                job_type="memory-intensive",
                cpu_request="500m",
                memory_request="4Gi",
                estimated_duration=20,
                image="busybox:1.35",
                command=[
                    "sh", "-c",
                    "echo 'Memory task started'; "
                    "for i in $(seq 1 20); do "
                    "  dd if=/dev/zero of=/tmp/testfile bs=100M count=5 2>/dev/null; "
                    "  sleep 30; "
                    "  rm -f /tmp/testfile; "
                    "done; "
                    "echo 'Memory task completed'"
                ],
                priority=3
            ),
            JobTemplate(
                name="mixed-task",
                job_type="mixed",
                cpu_request="800m",
                memory_request="3Gi",
                estimated_duration=10,
                image="busybox:1.35",
                command=[
                    "sh", "-c",
                    "echo 'Mixed task started'; "
                    "for i in $(seq 1 20); do "
                    "  echo 'Processing batch' $i; "
                    "  awk 'BEGIN {print sqrt(12345)}' > /tmp/result; "
                    "  sleep 30; "
                    "done; "
                    "echo 'Mixed task completed'"
                ],
                priority=4
            ),
            JobTemplate(
                name="quick-task",
                job_type="lightweight",
                cpu_request="200m",
                memory_request="512Mi",
                estimated_duration=5,
                image="busybox:1.35",
                command=[
                    "sh", "-c",
                    "echo 'Quick task started'; "
                    "sleep 300; "  # 5ë¶„
                    "echo 'Quick task completed'"
                ],
                priority=2
            )
        ]
        
    def create_test_job(self, template: JobTemplate) -> str:
        """í…ŒìŠ¤íŠ¸ ì‘ì—… YAML ìƒì„±"""
        self.job_counter += 1
        job_name = f"{template.name}-{self.job_counter:04d}"
        
        job_yaml = {
            'apiVersion': 'batch/v1',
            'kind': 'Job',
            'metadata': {
                'name': job_name,
                'namespace': self.namespace,
                'labels': {
                    'app': 'skrueue-test',
                    'job-type': template.job_type,
                    'scheduler': 'skrueue',
                    'priority': str(template.priority)
                },
                'annotations': {
                    'skrueue.ai/estimated-duration': str(template.estimated_duration),
                    'skrueue.ai/job-type': template.job_type
                }
            },
            'spec': {
                'suspend': True,  # RL ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ê´€ë¦¬í•˜ë„ë¡ ì¼ì‹œì •ì§€
                'backoffLimit': 2,
                'activeDeadlineSeconds': template.estimated_duration * 60 * 3,  # 3ë°° ì—¬ìœ 
                'template': {
                    'metadata': {
                        'labels': {
                            'app': 'skrueue-test',
                            'job-name': job_name
                        }
                    },
                    'spec': {
                        'restartPolicy': 'Never',
                        'containers': [{
                            'name': 'worker',
                            'image': template.image,
                            'command': template.command,
                            'resources': {
                                'requests': {
                                    'cpu': template.cpu_request,
                                    'memory': template.memory_request
                                },
                                'limits': {
                                    'cpu': template.cpu_request,
                                    'memory': template.memory_request
                                }
                            }
                        }]
                    }
                }
            }
        }
        
        return yaml.dump(job_yaml, default_flow_style=False)
        
    def submit_job(self, template: JobTemplate) -> bool:
        """ì‘ì—…ì„ í´ëŸ¬ìŠ¤í„°ì— ì œì¶œ"""
        try:
            job_yaml = self.create_test_job(template)
            
            process = subprocess.Popen(
                ['kubectl', 'apply', '-f', '-'],
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            stdout, stderr = process.communicate(input=job_yaml)
            
            if process.returncode == 0:
                self.logger.info(f"ì‘ì—… ì œì¶œ ì„±ê³µ: {template.name}-{self.job_counter:04d}")
                return True
            else:
                self.logger.error(f"ì‘ì—… ì œì¶œ ì‹¤íŒ¨: {stderr}")
                return False
                
        except Exception as e:
            self.logger.error(f"ì‘ì—… ì œì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
            
    def generate_random_workload(self) -> JobTemplate:
        """ëœë¤í•œ ì›Œí¬ë¡œë“œ ìƒì„±"""
        import random
        
        # ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ í…œí”Œë¦¿ ì„ íƒ
        weights = [0.3, 0.3, 0.2, 0.2]  # ê· ë“±í•˜ê²Œ ë¶„ë°°
        template = random.choices(self.job_templates, weights=weights)[0]
        
        # ì•½ê°„ì˜ ë³€ë™ì„± ì¶”ê°€
        variance_factor = random.uniform(0.8, 1.2)
        
        new_template = JobTemplate(
            name=template.name,
            job_type=template.job_type,
            cpu_request=template.cpu_request,
            memory_request=template.memory_request,
            estimated_duration=int(template.estimated_duration * variance_factor),
            image=template.image,
            command=template.command,
            priority=template.priority
        )
        
        return new_template

class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, namespace: str = "skrueue-test"):
        self.namespace = namespace
        self.logger = logging.getLogger('PerformanceMonitor')
        self.monitoring_active = False
        self.metrics_data = []
        
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.monitoring_active = True
        self.metrics_data.clear()
        
        monitoring_thread = threading.Thread(target=self._monitoring_loop)
        monitoring_thread.daemon = True
        monitoring_thread.start()
        
        self.logger.info("ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring_active = False
        self.logger.info("ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
        
    def _monitoring_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë©”ì¸ ë£¨í”„"""
        while self.monitoring_active:
            try:
                metrics = self._collect_metrics()
                if metrics:
                    self.metrics_data.append(metrics)
                time.sleep(30)  # 30ì´ˆë§ˆë‹¤ ìˆ˜ì§‘
                
            except Exception as e:
                self.logger.error(f"ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                
    def _collect_metrics(self) -> Dict:
        """í˜„ì¬ ì‹œì ì˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            # ì‘ì—… ìƒíƒœ ì¡°íšŒ
            jobs_result = subprocess.run(
                ['kubectl', 'get', 'jobs', '-n', self.namespace, '-o', 'json'],
                capture_output=True, text=True
            )
            
            # íŒŸ ìƒíƒœ ì¡°íšŒ  
            pods_result = subprocess.run(
                ['kubectl', 'get', 'pods', '-n', self.namespace, '-o', 'json'],
                capture_output=True, text=True
            )
            
            # ë…¸ë“œ ë¦¬ì†ŒìŠ¤ ìƒíƒœ ì¡°íšŒ
            try:
                nodes_result = subprocess.run(
                    ['kubectl', 'get', 'nodes', '-o', 'json'],
                    capture_output=True, text=True
                )
            except Exception:
                nodes_result = None
                
            timestamp = datetime.now()
            
            # JSON íŒŒì‹±
            jobs_data = json.loads(jobs_result.stdout) if jobs_result.returncode == 0 else {'items': []}
            pods_data = json.loads(pods_result.stdout) if pods_result.returncode == 0 else {'items': []}
            
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            pending_jobs = 0
            running_jobs = 0
            completed_jobs = 0
            failed_jobs = 0
            
            for job in jobs_data['items']:
                status = job.get('status', {})
                if status.get('active'):
                    running_jobs += status['active']
                elif status.get('succeeded'):
                    completed_jobs += status['succeeded']
                elif status.get('failed'):
                    failed_jobs += status['failed']
                elif job.get('spec', {}).get('suspend', False):
                    pending_jobs += 1
            
            # OOM íŒŸ ì¹´ìš´íŠ¸
            oom_pods = sum(1 for pod in pods_data['items']
                          if self._is_pod_oom_killed(pod))
            
            # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ê³„ì‚° (ê°„ì†Œí™”)
            cpu_utilization = min(running_jobs * 0.1, 1.0)  # ê·¼ì‚¬ì¹˜
            memory_utilization = min(running_jobs * 0.15, 1.0)  # ê·¼ì‚¬ì¹˜
            
            return {
                'timestamp': timestamp,
                'pending_jobs': pending_jobs,
                'running_jobs': running_jobs,
                'completed_jobs': completed_jobs,
                'failed_jobs': failed_jobs,
                'oom_incidents': oom_pods,
                'total_pods': len(pods_data['items']),
                'cpu_utilization': cpu_utilization,
                'memory_utilization': memory_utilization
            }
            
        except Exception as e:
            self.logger.error(f"ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return None
            
    def _is_pod_oom_killed(self, pod: Dict) -> bool:
        """íŒŸì´ OOMìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        try:
            status = pod.get('status', {})
            container_statuses = status.get('containerStatuses', [])
            
            for container_status in container_statuses:
                # í˜„ì¬ ìƒíƒœ í™•ì¸
                terminated = container_status.get('state', {}).get('terminated', {})
                if terminated.get('reason') == 'OOMKilled':
                    return True
                    
                # ì´ì „ ìƒíƒœ í™•ì¸
                last_state = container_status.get('lastState', {}).get('terminated', {})
                if last_state.get('reason') == 'OOMKilled':
                    return True
                    
        except Exception:
            pass
            
        return False
        
    def get_metrics_summary(self) -> Dict:
        """ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­ ìš”ì•½"""
        if not self.metrics_data:
            return {
                'total_samples': 0,
                'avg_pending_jobs': 0,
                'avg_running_jobs': 0,
                'total_completed': 0,
                'total_failed': 0,
                'total_oom_incidents': 0,
                'peak_concurrent_jobs': 0,
                'avg_cpu_utilization': 0,
                'avg_memory_utilization': 0
            }
            
        df = pd.DataFrame(self.metrics_data)
        
        return {
            'total_samples': len(df),
            'avg_pending_jobs': df['pending_jobs'].mean(),
            'avg_running_jobs': df['running_jobs'].mean(),
            'total_completed': df['completed_jobs'].iloc[-1] if len(df) > 0 else 0,
            'total_failed': df['failed_jobs'].iloc[-1] if len(df) > 0 else 0,
            'total_oom_incidents': df['oom_incidents'].sum(),
            'peak_concurrent_jobs': df['running_jobs'].max(),
            'avg_cpu_utilization': df['cpu_utilization'].mean(),
            'avg_memory_utilization': df['memory_utilization'].mean()
        }

class SKRueueTester:
    """SKRueue í†µí•© í…ŒìŠ¤í„° (ê°œì„ ëœ ë²„ì „)"""
    
    def __init__(self, config: TestConfiguration):
        self.config = config
        self.logger = logging.getLogger('SKRueueTester')
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.job_generator = JobGenerator(config.namespace)
        self.performance_monitor = PerformanceMonitor(config.namespace)
        
        # ê²°ê³¼ ì €ì¥
        self.test_results: Dict[str, TestResults] = {}
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(config.output_dir, exist_ok=True)
        
        # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
        os.makedirs("models", exist_ok=True)
        
    def setup_test_environment(self):
        """í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •"""
        self.logger.info("í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì¤‘...")
        
        try:
            # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
            subprocess.run([
                'kubectl', 'create', 'namespace', self.config.namespace
            ], check=False)
            
            # ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ì‘ì—… ì •ë¦¬
            subprocess.run([
                'kubectl', 'delete', 'jobs', '--all', 
                '-n', self.config.namespace
            ], check=False)
            
            time.sleep(10)  # ì •ë¦¬ ì™„ë£Œ ëŒ€ê¸°
            
            self.logger.info("í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì‹¤íŒ¨: {e}")
            raise
            
    def train_rl_models(self):
        """RL ëª¨ë¸ë“¤ í›ˆë ¨"""
        self.logger.info("ğŸ¯ RL ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        
        trained_models = {}
        
        for algorithm in self.config.algorithms_to_test:
            try:
                self.logger.info(f"ğŸ¤– {algorithm} ëª¨ë¸ í›ˆë ¨ ì¤‘...")
                
                # í™˜ê²½ ìƒì„±
                env = create_skrueue_environment(
                    [self.config.namespace], 
                    self.config.max_queue_size
                )
                
                # ì—ì´ì „íŠ¸ ìƒì„± ë° í›ˆë ¨
                agent = RLAgent(env, algorithm)
                agent.train(total_timesteps=self.config.model_training_steps)
                
                # ëª¨ë¸ ì €ì¥
                model_path = f"models/skrueue_{algorithm.lower()}_trained.zip"
                agent.save_model(model_path)
                
                trained_models[algorithm] = model_path
                
                self.logger.info(f"âœ… {algorithm} í›ˆë ¨ ì™„ë£Œ: {model_path}")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del env, agent
                time.sleep(5)
                
            except Exception as e:
                self.logger.error(f"âŒ {algorithm} í›ˆë ¨ ì‹¤íŒ¨: {e}")
                
        return trained_models
        
    def run_baseline_test(self) -> TestResults:
        """ê¸°ì¤€ ìŠ¤ì¼€ì¤„ëŸ¬ í…ŒìŠ¤íŠ¸ (ì‘ì—…ì´ suspend=falseë¡œ ìë™ ì‹¤í–‰)"""
        self.logger.info("ğŸ“Š ê¸°ì¤€ ìŠ¤ì¼€ì¤„ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.performance_monitor.start_monitoring()
        
        start_time = time.time()
        submitted_jobs = 0
        
        try:
            # ê¸°ì¤€ í…ŒìŠ¤íŠ¸ìš© ì‘ì—…ë“¤ì„ suspend=falseë¡œ ì œì¶œ
            while time.time() - start_time < self.config.test_duration_minutes * 60:
                template = self.job_generator.generate_random_workload()
                
                # ê¸°ì¤€ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ suspend=false ì‘ì—… ìƒì„±
                job_yaml = self._create_baseline_job(template)
                
                if self._submit_yaml(job_yaml):
                    submitted_jobs += 1
                    
                time.sleep(self.config.job_submission_interval)
                
                if submitted_jobs % 5 == 0:
                    self._wait_for_capacity()
                    
        except KeyboardInterrupt:
            self.logger.info("ì‚¬ìš©ìì— ì˜í•´ í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
            
        finally:
            self.performance_monitor.stop_monitoring()
            
        # ê²°ê³¼ ë¶„ì„
        results = self._analyze_results(self.config.baseline_scheduler, submitted_jobs)
        self.test_results[self.config.baseline_scheduler] = results
        
        return results
        
    def run_rl_scheduler_test(self, algorithm: str, model_path: str) -> TestResults:
        """RL ìŠ¤ì¼€ì¤„ëŸ¬ í…ŒìŠ¤íŠ¸"""
        self.logger.info(f"ğŸ¤– {algorithm} RL ìŠ¤ì¼€ì¤„ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # RL í™˜ê²½ ë° ì—ì´ì „íŠ¸ ì„¤ì •
        try:
            env = create_skrueue_environment([self.config.namespace], self.config.max_queue_size)
            agent = RLAgent(env, algorithm)
            agent.load_model(model_path)
            
            self.logger.info(f"âœ… {algorithm} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
            
        except Exception as e:
            self.logger.error(f"âŒ RL ì—ì´ì „íŠ¸ ì„¤ì • ì‹¤íŒ¨: {e}")
            return None
            
        # ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.performance_monitor.start_monitoring()
        
        start_time = time.time()
        submitted_jobs = 0
        scheduler_decisions = []
        
        # RL ì—ì´ì „íŠ¸ ì‹¤í–‰ ìŠ¤ë ˆë“œ
        def run_rl_agent():
            obs, info = env.reset()
            while time.time() - start_time < self.config.test_duration_minutes * 60:
                try:
                    action = agent.predict(obs)
                    obs, reward, terminated, truncated, info = env.step(action)
                    
                    # ê²°ì • ê¸°ë¡
                    scheduler_decisions.append({
                        'timestamp': datetime.now().isoformat(),
                        'action': int(action),
                        'reward': float(reward),
                        'queue_length': info.get('queue_length', 0),
                        'cluster_utilization': info.get('cluster_utilization', 0)
                    })
                    
                    if terminated or truncated:
                        obs, info = env.reset()
                        
                    time.sleep(5)  # 5ì´ˆë§ˆë‹¤ ê²°ì •
                    
                except Exception as e:
                    self.logger.error(f"RL ì—ì´ì „íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                    break
                    
        # RL ì—ì´ì „íŠ¸ë¥¼ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        agent_thread = threading.Thread(target=run_rl_agent)
        agent_thread.daemon = True
        agent_thread.start()
        
        try:
            # ì‘ì—… ì œì¶œ ë£¨í”„ (suspend=trueë¡œ ì œì¶œ)
            while time.time() - start_time < self.config.test_duration_minutes * 60:
                template = self.job_generator.generate_random_workload()
                
                if self.job_generator.submit_job(template):
                    submitted_jobs += 1
                    
                time.sleep(self.config.job_submission_interval)
                
                if submitted_jobs % 5 == 0:
                    self._wait_for_capacity()
                    
        except KeyboardInterrupt:
            self.logger.info("ì‚¬ìš©ìì— ì˜í•´ í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
            
        finally:
            self.performance_monitor.stop_monitoring()
            
        # ê²°ê³¼ ë¶„ì„
        results = self._analyze_results(f"SKRueue-{algorithm}", submitted_jobs, scheduler_decisions)
        self.test_results[f"SKRueue-{algorithm}"] = results
        
        return results
        
    def _create_baseline_job(self, template: JobTemplate) -> str:
        """ê¸°ì¤€ í…ŒìŠ¤íŠ¸ìš© ì‘ì—… ìƒì„± (suspend=false)"""
        job_counter = getattr(self, '_baseline_counter', 0) + 1
        setattr(self, '_baseline_counter', job_counter)
        
        job_name = f"baseline-{template.name}-{job_counter:04d}"
        
        job_yaml = {
            'apiVersion': 'batch/v1',
            'kind': 'Job',
            'metadata': {
                'name': job_name,
                'namespace': self.config.namespace,
                'labels': {
                    'app': 'skrueue-test',
                    'job-type': template.job_type,
                    'scheduler': 'baseline',
                    'priority': str(template.priority)
                },
                'annotations': {
                    'skrueue.ai/estimated-duration': str(template.estimated_duration),
                    'skrueue.ai/job-type': template.job_type
                }
            },
            'spec': {
                'suspend': False,  # ì¦‰ì‹œ ì‹¤í–‰
                'backoffLimit': 2,
                'activeDeadlineSeconds': template.estimated_duration * 60 * 3,
                'template': {
                    'metadata': {
                        'labels': {
                            'app': 'skrueue-test',
                            'job-name': job_name
                        }
                    },
                    'spec': {
                        'restartPolicy': 'Never',
                        'containers': [{
                            'name': 'worker',
                            'image': template.image,
                            'command': template.command,
                            'resources': {
                                'requests': {
                                    'cpu': template.cpu_request,
                                    'memory': template.memory_request
                                },
                                'limits': {
                                    'cpu': template.cpu_request,
                                    'memory': template.memory_request
                                }
                            }
                        }]
                    }
                }
            }
        }
        
        return yaml.dump(job_yaml, default_flow_style=False)
        
    def _submit_yaml(self, job_yaml: str) -> bool:
        """YAML ì œì¶œ"""
        try:
            process = subprocess.Popen(
                ['kubectl', 'apply', '-f', '-'],
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            stdout, stderr = process.communicate(input=job_yaml)
            
            return process.returncode == 0
            
        except Exception as e:
            self.logger.error(f"YAML ì œì¶œ ì‹¤íŒ¨: {e}")
            return False
        
    def _wait_for_capacity(self):
        """í´ëŸ¬ìŠ¤í„° ìš©ëŸ‰ ëŒ€ê¸°"""
        try:
            result = subprocess.run([
                'kubectl', 'get', 'jobs', '-n', self.config.namespace,
                '--field-selector=status.active=1', '--no-headers'
            ], capture_output=True, text=True)
            
            running_count = len(result.stdout.strip().split('\n')) if result.stdout.strip() else 0
            
            while running_count >= self.config.max_concurrent_jobs:
                time.sleep(30)
                
                result = subprocess.run([
                    'kubectl', 'get', 'jobs', '-n', self.config.namespace,
                    '--field-selector=status.active=1', '--no-headers'
                ], capture_output=True, text=True)
                
                running_count = len(result.stdout.strip().split('\n')) if result.stdout.strip() else 0
                
        except Exception as e:
            self.logger.warning(f"ìš©ëŸ‰ í™•ì¸ ì‹¤íŒ¨: {e}")
            
    def _analyze_results(self, scheduler_name: str, submitted_jobs: int, 
                        scheduler_decisions: List[Dict] = None) -> TestResults:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„"""
        try:
            # ë©”íŠ¸ë¦­ ìš”ì•½
            metrics_summary = self.performance_monitor.get_metrics_summary()
            
            # ì‘ì—… ìƒíƒœ ì¡°íšŒ
            jobs_result = subprocess.run([
                'kubectl', 'get', 'jobs', '-n', self.config.namespace, '-o', 'json'
            ], capture_output=True, text=True)
            
            if jobs_result.returncode != 0:
                raise Exception("ì‘ì—… ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨")
                
            jobs_data = json.loads(jobs_result.stdout)
            
            # ê²°ê³¼ ê³„ì‚°
            completed_jobs = 0
            failed_jobs = 0
            total_wait_time = 0
            total_execution_time = 0
            
            for job in jobs_data['items']:
                # ìŠ¤ì¼€ì¤„ëŸ¬ë³„ í•„í„°ë§
                labels = job.get('metadata', {}).get('labels', {})
                if scheduler_name == "kueue-default" and labels.get('scheduler') != 'baseline':
                    continue
                elif scheduler_name.startswith("SKRueue") and labels.get('scheduler') != 'skrueue':
                    continue
                    
                status = job.get('status', {})
                
                if status.get('succeeded'):
                    completed_jobs += status['succeeded']
                elif status.get('failed'):
                    failed_jobs += status['failed']
                    
                # ì‹œê°„ ê³„ì‚°
                creation_time = job['metadata'].get('creationTimestamp')
                start_time = status.get('startTime')
                completion_time = status.get('completionTime')
                
                if creation_time and start_time:
                    creation_dt = datetime.fromisoformat(creation_time.replace('Z', '+00:00'))
                    start_dt = datetime.fromisoformat(start_time.replace('Z', '+00:00'))
                    wait_time = (start_dt - creation_dt).total_seconds() / 60.0
                    total_wait_time += wait_time
                    
                if start_time and completion_time:
                    start_dt = datetime.fromisoformat(start_time.replace('Z', '+00:00'))
                    completion_dt = datetime.fromisoformat(completion_time.replace('Z', '+00:00'))
                    execution_time = (completion_dt - start_dt).total_seconds() / 60.0
                    total_execution_time += execution_time
                    
            # í‰ê·  ê³„ì‚°
            total_finished = completed_jobs + failed_jobs
            avg_wait_time = total_wait_time / max(total_finished, 1)
            avg_execution_time = total_execution_time / max(completed_jobs, 1)
            
            # ì²˜ë¦¬ëŸ‰ ë° ì„±ê³µë¥  ê³„ì‚°
            throughput = completed_jobs / (self.config.test_duration_minutes / 60.0)
            success_rate = (completed_jobs / max(submitted_jobs, 1)) * 100
            
            return TestResults(
                scheduler_name=scheduler_name,
                total_jobs_submitted=submitted_jobs,
                total_jobs_completed=completed_jobs,
                total_jobs_failed=failed_jobs,
                average_wait_time=avg_wait_time,
                average_execution_time=avg_execution_time,
                resource_utilization={
                    'cpu': metrics_summary.get('avg_cpu_utilization', 0) * 100,
                    'memory': metrics_summary.get('avg_memory_utilization', 0) * 100
                },
                oom_incidents=metrics_summary.get('total_oom_incidents', 0),
                throughput=throughput,
                success_rate=success_rate,
                scheduler_decisions=scheduler_decisions or []
            )
            
        except Exception as e:
            self.logger.error(f"ê²°ê³¼ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None
            
    def run_full_comparison_test(self):
        """ì „ì²´ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.logger.info("ğŸš€ SKRueue ì „ì²´ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        try:
            # 1. í™˜ê²½ ì„¤ì •
            self.setup_test_environment()
            
            # 2. RL ëª¨ë¸ í›ˆë ¨
            trained_models = self.train_rl_models()
            
            if not trained_models:
                self.logger.error("âŒ í›ˆë ¨ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return
                
            # 3. ê¸°ì¤€ ìŠ¤ì¼€ì¤„ëŸ¬ í…ŒìŠ¤íŠ¸
            self.logger.info("\n" + "="*60)
            self.logger.info("ğŸ“Š ê¸°ì¤€ ìŠ¤ì¼€ì¤„ëŸ¬ í…ŒìŠ¤íŠ¸")
            self.logger.info("="*60)
            
            self.setup_test_environment()
            time.sleep(10)
            baseline_results = self.run_baseline_test()
            
            # 4. RL ìŠ¤ì¼€ì¤„ëŸ¬ë“¤ í…ŒìŠ¤íŠ¸
            for algorithm in self.config.algorithms_to_test:
                if algorithm in trained_models:
                    self.logger.info(f"\n" + "="*60)
                    self.logger.info(f"ğŸ¤– {algorithm} RL ìŠ¤ì¼€ì¤„ëŸ¬ í…ŒìŠ¤íŠ¸")
                    self.logger.info("="*60)
                    
                    self.setup_test_environment()
                    time.sleep(10)
                    
                    rl_results = self.run_rl_scheduler_test(algorithm, trained_models[algorithm])
                    
                    if rl_results:
                        self.logger.info(f"âœ… {algorithm} í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
                    else:
                        self.logger.error(f"âŒ {algorithm} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                    
                    time.sleep(30)  # ë‹¤ìŒ í…ŒìŠ¤íŠ¸ ì „ ëŒ€ê¸°
                    
            # 5. ê²°ê³¼ ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„±
            self.generate_comparison_report()
            
        except Exception as e:
            self.logger.error(f"âŒ ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            
    def generate_comparison_report(self):
        """ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±"""
        self.logger.info("ğŸ“Š ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        try:
            # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±
            results_data = []
            
            for scheduler_name, results in self.test_results.items():
                if results:
                    results_data.append({
                        'Scheduler': scheduler_name,
                        'Throughput (jobs/hour)': f"{results.throughput:.2f}",
                        'Avg Wait Time (min)': f"{results.average_wait_time:.2f}",
                        'Success Rate (%)': f"{results.success_rate:.1f}",
                        'OOM Incidents (count)': results.oom_incidents,
                        'CPU Utilization (%)': f"{results.resource_utilization['cpu']:.1f}",
                        'Memory Utilization (%)': f"{results.resource_utilization['memory']:.1f}",
                        'Jobs Submitted': results.total_jobs_submitted,
                        'Jobs Completed': results.total_jobs_completed,
                        'Jobs Failed': results.total_jobs_failed
                    })
                    
            if not results_data:
                self.logger.error("âŒ ìƒì„±í•  ê²°ê³¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
                    
            df = pd.DataFrame(results_data)
            
            # CSV ì €ì¥
            csv_path = os.path.join(self.config.output_dir, 'comparison_results.csv')
            df.to_csv(csv_path, index=False)
            
            # ì½˜ì†” ì¶œë ¥
            print("\n" + "="*80)
            print("ğŸ“Š SKRueue RL Model Comparison Results")
            print("="*80)
            print(df.to_string(index=False))
            print("="*80)
            
            # ì‹œê°í™” ìƒì„±
            self._create_performance_charts(df)
            
            # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
            self._generate_markdown_report(df)
            
            self.logger.info(f"âœ… ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {self.config.output_dir}")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            
    def _create_performance_charts(self, df: pd.DataFrame):
        """ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ ìƒì„±"""
        try:
            fig, axes = plt.subplots(2, 3, figsize=(18, 12))
            
            # ìˆ«ìí˜• ë°ì´í„°ë¡œ ë³€í™˜
            df_numeric = df.copy()
            numeric_cols = ['Throughput (jobs/hour)', 'Avg Wait Time (min)', 'Success Rate (%)', 
                          'CPU Utilization (%)', 'Memory Utilization (%)']
            
            for col in numeric_cols:
                df_numeric[col] = pd.to_numeric(df_numeric[col], errors='coerce')
            
            # 1. ì²˜ë¦¬ëŸ‰ ë¹„êµ
            axes[0, 0].bar(df_numeric['Scheduler'], df_numeric['Throughput (jobs/hour)'], color='skyblue')
            axes[0, 0].set_title('Throughput Comparison')
            axes[0, 0].set_ylabel('Jobs per Hour')
            axes[0, 0].tick_params(axis='x', rotation=45)
            
            # 2. í‰ê·  ëŒ€ê¸°ì‹œê°„ ë¹„êµ
            axes[0, 1].bar(df_numeric['Scheduler'], df_numeric['Avg Wait Time (min)'], color='lightcoral')
            axes[0, 1].set_title('Average Wait Time Comparison')
            axes[0, 1].set_ylabel('Minutes')
            axes[0, 1].tick_params(axis='x', rotation=45)
            
            # 3. ì„±ê³µë¥  ë¹„êµ
            axes[0, 2].bar(df_numeric['Scheduler'], df_numeric['Success Rate (%)'], color='lightgreen')
            axes[0, 2].set_title('Job Success Rate Comparison')
            axes[0, 2].set_ylabel('Success Rate (%)')
            axes[0, 2].tick_params(axis='x', rotation=45)
            
            # 4. OOM ì‚¬ê±´ ìˆ˜ ë¹„êµ
            axes[1, 0].bar(df['Scheduler'], df['OOM Incidents (count)'], color='orange')
            axes[1, 0].set_title('OOM Incidents Comparison')
            axes[1, 0].set_ylabel('Number of OOM Incidents')
            axes[1, 0].tick_params(axis='x', rotation=45)
            
            # 5. CPU ì‚¬ìš©ë¥  ë¹„êµ
            axes[1, 1].bar(df_numeric['Scheduler'], df_numeric['CPU Utilization (%)'], color='gold')
            axes[1, 1].set_title('CPU Utilization Comparison')
            axes[1, 1].set_ylabel('CPU Utilization (%)')
            axes[1, 1].tick_params(axis='x', rotation=45)
            
            # 6. ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë¹„êµ
            axes[1, 2].bar(df_numeric['Scheduler'], df_numeric['Memory Utilization (%)'], color='plum')
            axes[1, 2].set_title('Memory Utilization Comparison')
            axes[1, 2].set_ylabel('Memory Utilization (%)')
            axes[1, 2].tick_params(axis='x', rotation=45)
            
            plt.tight_layout()
            
            chart_path = os.path.join(self.config.output_dir, 'performance_comparison.png')
            plt.savefig(chart_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"ğŸ“ˆ ì„±ëŠ¥ ì°¨íŠ¸ ì €ì¥: {chart_path}")
            
        except Exception as e:
            self.logger.error(f"âŒ ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            
    def _generate_markdown_report(self, df: pd.DataFrame):
        """ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            report_path = os.path.join(self.config.output_dir, 'SKRueue_Test_Report.md')
            
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("# SKRueue Performance Test Report\n\n")
                f.write(f"**Test Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write(f"**Test Configuration:**\n")
                f.write(f"- Duration: {self.config.test_duration_minutes} minutes\n")
                f.write(f"- Job Submission Interval: {self.config.job_submission_interval} seconds\n")
                f.write(f"- Max Concurrent Jobs: {self.config.max_concurrent_jobs}\n")
                f.write(f"- Training Steps: {self.config.model_training_steps}\n")
                f.write(f"- Algorithms Tested: {', '.join(self.config.algorithms_to_test)}\n\n")
                
                f.write("## Test Results Summary\n\n")
                f.write(df.to_markdown(index=False))
                f.write("\n\n")
                
                f.write("## Key Findings\n\n")
                
                # ìµœê³  ì„±ëŠ¥ ìŠ¤ì¼€ì¤„ëŸ¬ ì°¾ê¸°
                df_numeric = df.copy()
                for col in ['Throughput (jobs/hour)', 'Avg Wait Time (min)', 'Success Rate (%)']:
                    df_numeric[col] = pd.to_numeric(df_numeric[col], errors='coerce')
                
                best_throughput = df_numeric.loc[df_numeric['Throughput (jobs/hour)'].idxmax(), 'Scheduler']
                best_wait_time = df_numeric.loc[df_numeric['Avg Wait Time (min)'].idxmin(), 'Scheduler']
                best_success_rate = df_numeric.loc[df_numeric['Success Rate (%)'].idxmax(), 'Scheduler']
                
                f.write(f"- **Best Throughput:** {best_throughput}\n")
                f.write(f"- **Lowest Wait Time:** {best_wait_time}\n")
                f.write(f"- **Highest Success Rate:** {best_success_rate}\n\n")
                
                f.write("## Performance Charts\n\n")
                f.write("![Performance Comparison](performance_comparison.png)\n\n")
                
                f.write("## Conclusion\n\n")
                f.write("ì´ í…ŒìŠ¤íŠ¸ëŠ” SKRueue RL ê¸°ë°˜ ìŠ¤ì¼€ì¤„ëŸ¬ì˜ ì„±ëŠ¥ì„ ê¸°ì¡´ ìŠ¤ì¼€ì¤„ëŸ¬ì™€ ë¹„êµí•œ ê²°ê³¼ì…ë‹ˆë‹¤. ")
                f.write("67ì°¨ì› ìƒíƒœ ê³µê°„ì„ í™œìš©í•œ ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ë“¤ì˜ ì„±ëŠ¥ì„ ì‹¤ì œ ì›Œí¬ë¡œë“œë¡œ ê²€ì¦í–ˆìŠµë‹ˆë‹¤.\n")
                
            self.logger.info(f"ğŸ“ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
            
        except Exception as e:
            self.logger.error(f"âŒ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='SKRueue í†µí•© í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ')
    parser.add_argument('--duration', type=int, default=15, help='í…ŒìŠ¤íŠ¸ ì§€ì† ì‹œê°„ (ë¶„)')
    parser.add_argument('--algorithms', nargs='*', default=['DQN', 'PPO', 'A2C'], 
                       help='í…ŒìŠ¤íŠ¸í•  RL ì•Œê³ ë¦¬ì¦˜')
    parser.add_argument('--namespace', type=str, default='skrueue-test', 
                       help='í…ŒìŠ¤íŠ¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤')
    parser.add_argument('--output-dir', type=str, default='skrueue_test_results',
                       help='ê²°ê³¼ ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--job-interval', type=int, default=20,
                       help='ì‘ì—… ì œì¶œ ê°„ê²© (ì´ˆ)')
    parser.add_argument('--training-steps', type=int, default=5000,
                       help='RL ëª¨ë¸ í›ˆë ¨ ìŠ¤í… ìˆ˜')
    parser.add_argument('--max-queue-size', type=int, default=10,
                       help='ìµœëŒ€ í í¬ê¸°')
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('skrueue_integration_test.log'),
            logging.StreamHandler()
        ]
    )
    
    # í…ŒìŠ¤íŠ¸ ì„¤ì •
    config = TestConfiguration(
        test_duration_minutes=args.duration,
        algorithms_to_test=args.algorithms,
        namespace=args.namespace,
        output_dir=args.output_dir,
        job_submission_interval=args.job_interval,
        model_training_steps=args.training_steps,
        max_queue_size=args.max_queue_size
    )
    
    # í…ŒìŠ¤í„° ìƒì„± ë° ì‹¤í–‰
    tester = SKRueueTester(config)
    
    try:
        print("ğŸš€ SKRueue í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        print(f"â±ï¸  í…ŒìŠ¤íŠ¸ ì‹œê°„: {args.duration}ë¶„")
        print(f"ğŸ¤– ì•Œê³ ë¦¬ì¦˜: {', '.join(args.algorithms)}")
        print(f"ğŸ¯ í›ˆë ¨ ìŠ¤í…: {args.training_steps}")
        print(f"ğŸ“‚ ê²°ê³¼ ë””ë ‰í† ë¦¬: {args.output_dir}")
        print("="*60)
        
        tester.run_full_comparison_test()
        
        print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ê²°ê³¼ëŠ” {args.output_dir} ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")
        print(f"ğŸ“Š CSV: {args.output_dir}/comparison_results.csv")
        print(f"ğŸ“ˆ ì°¨íŠ¸: {args.output_dir}/performance_comparison.png")
        print(f"ğŸ“ ë¦¬í¬íŠ¸: {args.output_dir}/SKRueue_Test_Report.md")
        
    except KeyboardInterrupt:
        print("\nâŒ ì‚¬ìš©ìì— ì˜í•´ í…ŒìŠ¤íŠ¸ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()