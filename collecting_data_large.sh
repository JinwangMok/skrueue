#!/bin/bash
# collecting_data_large.sh
# SKRueue 24ì‹œê°„ ëŒ€ìš©ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ
# í˜„ì‹¤ì ì¸ ì›Œí¬ë¡œë“œ íŒ¨í„´ìœ¼ë¡œ ë‹¤ì–‘í•œ ì‘ì—… ìƒì„± ë° ì¥ê¸°ê°„ ë°ì´í„° ìˆ˜ì§‘

# ë³¸ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ì†Œê°œ
# 1. í˜„ì‹¤ì ì¸ 24ì‹œê°„ ì›Œí¬ë¡œë“œ íŒ¨í„´
## ì‹œê°„ëŒ€ë³„ ë‹¤ë¥¸ íŒ¨í„´
# - ìƒˆë²½ (0-6ì‹œ): ë°°ì¹˜ ì‘ì—… ì¤‘ì‹¬ (ì ì€ ì‘ì—…ëŸ‰)
# - ì—…ë¬´ ì‹œê°„ (9-18ì‹œ): ë‹¤ì–‘í•œ ë¶„ì„ ì‘ì—… (ë§ì€ ì‘ì—…ëŸ‰)  
# - ì €ë… (19-23ì‹œ): ë³´ê³ ì„œ ìƒì„± (ì¤‘ê°„ ì‘ì—…ëŸ‰)
# 2. ë‹¤ì¤‘ ì›Œí¬ë¡œë“œ ìƒì„±
# # ë™ì‹œ ì‹¤í–‰ë˜ëŠ” ì›Œí¬ë¡œë“œ ìƒì„±ê¸°ë“¤
# - ë©”ì¸ ì‹œë®¬ë ˆì´ì…˜ (24ì‹œê°„ ì§€ì†)
# - ì‹œê°„ë‹¹ ì¶”ê°€ ë°°ì¹˜ ì‘ì—… (10-19ê°œ)
# - 6ê°€ì§€ ë‹¤ì–‘í•œ ì‘ì—… íƒ€ì…
# 3. ì™„ì „í•œ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
## ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
# - 5ë¶„ë§ˆë‹¤ ìƒíƒœ í™•ì¸
# - 1ì‹œê°„ë§ˆë‹¤ ìƒì„¸ ë¦¬í¬íŠ¸
# - ìë™ ë°±ì—… (1ì‹œê°„ë§ˆë‹¤)
# - í”„ë¡œì„¸ìŠ¤ ì¥ì•  ë³µêµ¬
# 4. ì•ˆì „í•œ ì‹¤í–‰ ê´€ë¦¬
## ì¥ì•  ëŒ€ì‘
# - í”„ë¡œì„¸ìŠ¤ ìë™ ì¬ì‹œì‘
# - ì‹ í˜¸ ì²˜ë¦¬ (Ctrl+C ì•ˆì „ ì¢…ë£Œ)
# - ìë™ ë°±ì—… ë° ë³µêµ¬
# - ìƒì„¸í•œ ë¡œê¹…
# ğŸ“Š ì˜ˆìƒ ë°ì´í„° ìƒì„±ëŸ‰
## 24ì‹œê°„ ì‹¤í–‰ ì‹œ ì˜ˆìƒ:
# â”œâ”€â”€ ì‘ì—… ìˆ˜: 1,000-3,000ê°œ (ì‹œê°„ëŒ€ë³„ ë³€ë™)
# â”œâ”€â”€ í´ëŸ¬ìŠ¤í„° ìƒíƒœ: 8,640ê°œ (10ì´ˆë§ˆë‹¤)
# â”œâ”€â”€ ë°ì´í„°ë² ì´ìŠ¤: 50-200MB
# â””â”€â”€ CSV í›ˆë ¨ ë°ì´í„°: 10,000-50,000 ë ˆì½”ë“œ


set -e  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ìŠ¤í¬ë¦½íŠ¸ ì¤‘ë‹¨

# ì„¤ì • ë³€ìˆ˜
DURATION_HOURS=24
DATA_COLLECTION_INTERVAL=10
NAMESPACE="skrueue-test"
LOG_DIR="logs_$(date +%Y%m%d_%H%M%S)"
DATA_OUTPUT_DIR="data_$(date +%Y%m%d_%H%M%S)"
BACKUP_INTERVAL=3600  # 1ì‹œê°„ë§ˆë‹¤ ë°±ì—…

# ìƒ‰ìƒ ì½”ë“œ
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# ë¡œê¹… í•¨ìˆ˜
log() {
    echo -e "${CYAN}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} $1" | tee -a "${LOG_DIR}/main.log"
}

error() {
    echo -e "${RED}[$(date '+%Y-%m-%d %H:%M:%S')] ERROR:${NC} $1" | tee -a "${LOG_DIR}/main.log"
}

success() {
    echo -e "${GREEN}[$(date '+%Y-%m-%d %H:%M:%S')] SUCCESS:${NC} $1" | tee -a "${LOG_DIR}/main.log"
}

warning() {
    echo -e "${YELLOW}[$(date '+%Y-%m-%d %H:%M:%S')] WARNING:${NC} $1" | tee -a "${LOG_DIR}/main.log"
}

# ë””ë ‰í† ë¦¬ ë° í™˜ê²½ ì„¤ì •
setup_environment() {
    log "ğŸ”§ 24ì‹œê°„ ëŒ€ìš©ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ í™˜ê²½ ì„¤ì • ì‹œì‘"
    
    # ë¡œê·¸ ë° ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
    mkdir -p "${LOG_DIR}"
    mkdir -p "${DATA_OUTPUT_DIR}"
    mkdir -p "backups"
    
    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í™•ì¸/ìƒì„±
    if ! kubectl get namespace "${NAMESPACE}" >/dev/null 2>&1; then
        log "ğŸ“‹ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„± ì¤‘: ${NAMESPACE}"
        kubectl create namespace "${NAMESPACE}"
    else
        log "âœ… ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í™•ì¸: ${NAMESPACE}"
    fi
    
    # ServiceAccount ë° ê¶Œí•œ ì„¤ì •
    log "ğŸ” ServiceAccount ë° ê¶Œí•œ ì„¤ì • ì¤‘..."
    kubectl apply -f - <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: skrueue-agent
  namespace: ${NAMESPACE}
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: skrueue-agent-role
rules:
- apiGroups: [""]
  resources: ["nodes", "pods", "events", "services", "configmaps"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["batch"]
  resources: ["jobs"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["sparkoperator.k8s.io"]
  resources: ["sparkapplications"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["kueue.x-k8s.io"]
  resources: ["workloads", "localqueues", "clusterqueues"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: skrueue-agent-binding
subjects:
- kind: ServiceAccount
  name: skrueue-agent
  namespace: ${NAMESPACE}
roleRef:
  kind: ClusterRole
  name: skrueue-agent-role
  apiGroup: rbac.authorization.k8s.io
EOF
    
    success "âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ"
}

# ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
cleanup_processes() {
    log "ğŸ§¹ ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì¤‘..."
    
    # ì´ì „ ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ë“¤ ì¢…ë£Œ
    pkill -f "data_collector.py" 2>/dev/null || true
    pkill -f "sample_workload_generator.py" 2>/dev/null || true
    
    # ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ì‘ì—…ë“¤ ì •ë¦¬ (ì„ íƒì )
    read -p "ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ì‘ì—…ë“¤ì„ ì •ë¦¬í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): " cleanup_jobs
    if [[ $cleanup_jobs =~ ^[Yy]$ ]]; then
        log "ğŸ—‘ï¸ ê¸°ì¡´ ì‘ì—… ì •ë¦¬ ì¤‘..."
        kubectl delete jobs --all -n "${NAMESPACE}" --wait=false 2>/dev/null || true
        kubectl delete pods --all -n "${NAMESPACE}" --wait=false 2>/dev/null || true
        sleep 10
    fi
    
    success "âœ… í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì™„ë£Œ"
}

# í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
check_cluster_status() {
    log "ğŸ” í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸ ì¤‘..."
    
    # ë…¸ë“œ ìƒíƒœ í™•ì¸
    echo "ğŸ“Š í´ëŸ¬ìŠ¤í„° ë…¸ë“œ ìƒíƒœ:" | tee -a "${LOG_DIR}/cluster_status.log"
    kubectl get nodes -o wide | tee -a "${LOG_DIR}/cluster_status.log"
    
    # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸
    echo "ğŸ’¾ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰:" | tee -a "${LOG_DIR}/cluster_status.log"
    kubectl top nodes 2>/dev/null | tee -a "${LOG_DIR}/cluster_status.log" || log "âš ï¸ kubectl top ì‚¬ìš© ë¶ˆê°€ (metrics-server í•„ìš”)"
    
    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í˜„ì¬ ìƒíƒœ
    echo "ğŸ“‹ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ${NAMESPACE} ìƒíƒœ:" | tee -a "${LOG_DIR}/cluster_status.log"
    kubectl get all -n "${NAMESPACE}" | tee -a "${LOG_DIR}/cluster_status.log"
    
    success "âœ… í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸ ì™„ë£Œ"
}

# ë°ì´í„° ìˆ˜ì§‘ê¸° ì‹œì‘
start_data_collector() {
    log "ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ê¸° ì‹œì‘ ì¤‘..."
    
    # ë°ì´í„° ìˆ˜ì§‘ê¸° ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
    nohup python data_collector.py \
        --interval ${DATA_COLLECTION_INTERVAL} \
        --namespaces default ${NAMESPACE} monitoring kube-system \
        --retention-days 7 \
        --db-path "${DATA_OUTPUT_DIR}/skrueue_large_dataset.db" \
        > "${LOG_DIR}/data_collector.log" 2>&1 &
    
    DATA_COLLECTOR_PID=$!
    echo $DATA_COLLECTOR_PID > "${LOG_DIR}/data_collector.pid"
    
    log "ğŸ“Š Data Collector PID: $DATA_COLLECTOR_PID"
    
    # ë°ì´í„° ìˆ˜ì§‘ê¸° ì‹œì‘ í™•ì¸
    sleep 10
    if ps -p $DATA_COLLECTOR_PID > /dev/null; then
        success "âœ… ë°ì´í„° ìˆ˜ì§‘ê¸° ì •ìƒ ì‹œì‘"
    else
        error "âŒ ë°ì´í„° ìˆ˜ì§‘ê¸° ì‹œì‘ ì‹¤íŒ¨"
        cat "${LOG_DIR}/data_collector.log"
        exit 1
    fi
}

# ì›Œí¬ë¡œë“œ ìƒì„±ê¸° ì‹œì‘ (24ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜)
start_workload_generator() {
    log "ğŸš€ 24ì‹œê°„ ì›Œí¬ë¡œë“œ ìƒì„±ê¸° ì‹œì‘ ì¤‘..."
    
    # ì›Œí¬ë¡œë“œ ìƒì„±ê¸° ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (simulation ëª¨ë“œ)
    nohup python sample_workload_generator.py \
        --mode simulation \
        --duration ${DURATION_HOURS} \
        --namespace ${NAMESPACE} \
        --force-k8s-jobs \
        > "${LOG_DIR}/workload_generator.log" 2>&1 &
    
    WORKLOAD_GENERATOR_PID=$!
    echo $WORKLOAD_GENERATOR_PID > "${LOG_DIR}/workload_generator.pid"
    
    log "ğŸš€ Workload Generator PID: $WORKLOAD_GENERATOR_PID"
    
    # ì›Œí¬ë¡œë“œ ìƒì„±ê¸° ì‹œì‘ í™•ì¸
    sleep 15
    if ps -p $WORKLOAD_GENERATOR_PID > /dev/null; then
        success "âœ… ì›Œí¬ë¡œë“œ ìƒì„±ê¸° ì •ìƒ ì‹œì‘"
        
        # ì²« ì‘ì—… ìƒì„± í™•ì¸
        log "â³ ì²« ì‘ì—… ìƒì„± ëŒ€ê¸° ì¤‘... (ìµœëŒ€ 5ë¶„)"
        for i in {1..30}; do
            job_count=$(kubectl get jobs -n "${NAMESPACE}" --no-headers 2>/dev/null | wc -l)
            if [ "$job_count" -gt 0 ]; then
                success "âœ… ì²« ì‘ì—… ìƒì„± í™•ì¸: ${job_count}ê°œ"
                break
            fi
            sleep 10
        done
    else
        error "âŒ ì›Œí¬ë¡œë“œ ìƒì„±ê¸° ì‹œì‘ ì‹¤íŒ¨"
        cat "${LOG_DIR}/workload_generator.log"
        exit 1
    fi
}

# ì¶”ê°€ì ì¸ ë‹¤ì–‘í•œ ì›Œí¬ë¡œë“œ íŒ¨í„´ ìƒì„±
generate_additional_workloads() {
    log "ğŸ¯ ì¶”ê°€ ë‹¤ì–‘í•œ ì›Œí¬ë¡œë“œ íŒ¨í„´ ìƒì„± ì¤‘..."
    
    # 1ì‹œê°„ë§ˆë‹¤ ì¶”ê°€ ì›Œí¬ë¡œë“œ ìƒì„± (ë°±ê·¸ë¼ìš´ë“œ)
    (
        for hour in $(seq 1 ${DURATION_HOURS}); do
            sleep 3600  # 1ì‹œê°„ ëŒ€ê¸°
            
            log "â° ${hour}ì‹œê°„ ê²½ê³¼ - ì¶”ê°€ ì›Œí¬ë¡œë“œ ìƒì„±"
            
            # ì‹œê°„ëŒ€ì— ë”°ë¥¸ ë‹¤ë¥¸ íŒ¨í„´ ì ìš©
            current_hour=$(date +%H)
            
            if [ $current_hour -ge 9 ] && [ $current_hour -le 18 ]; then
                # ì—…ë¬´ ì‹œê°„: ë” ë§ì€ ì‘ì—…
                batch_size=$((RANDOM % 10 + 10))  # 10-19ê°œ
                interval=30
            elif [ $current_hour -ge 19 ] && [ $current_hour -le 23 ]; then
                # ì €ë… ì‹œê°„: ì¤‘ê°„ ì •ë„
                batch_size=$((RANDOM % 8 + 5))   # 5-12ê°œ
                interval=45
            else
                # ìƒˆë²½/ë°¤: ì ì€ ì‘ì—…
                batch_size=$((RANDOM % 5 + 3))   # 3-7ê°œ
                interval=60
            fi
            
            log "ğŸ“Š ì‹œê°„ëŒ€ë³„ ë°°ì¹˜ ì‘ì—… ìƒì„±: ${batch_size}ê°œ (ê°„ê²©: ${interval}ì´ˆ)"
            
            # ë°°ì¹˜ ì‘ì—… ìƒì„±
            timeout 1800 python sample_workload_generator.py \
                --mode batch \
                --count ${batch_size} \
                --interval ${interval} \
                --namespace ${NAMESPACE} \
                --force-k8s-jobs \
                >> "${LOG_DIR}/additional_workloads.log" 2>&1 &
                
        done
    ) &
    
    ADDITIONAL_WORKLOAD_PID=$!
    echo $ADDITIONAL_WORKLOAD_PID > "${LOG_DIR}/additional_workload.pid"
    
    log "ğŸ¯ Additional Workload Generator PID: $ADDITIONAL_WORKLOAD_PID"
}

# ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‹œì‘
start_monitoring() {
    log "ğŸ“ˆ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‹œì‘ ì¤‘..."
    
    # ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    cat > "${LOG_DIR}/monitor.sh" << 'EOF'
#!/bin/bash
LOG_DIR="$1"
NAMESPACE="$2"
DATA_OUTPUT_DIR="$3"

while true; do
    timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    # ì‘ì—… ìƒíƒœ ìˆ˜ì§‘
    job_stats=$(kubectl get jobs -n "$NAMESPACE" --no-headers 2>/dev/null | \
        awk 'BEGIN{pending=0; running=0; succeeded=0; failed=0} 
             {if($2=="0/1") pending++; 
              else if($3=="0") running++; 
              else if($2=="1/1") succeeded++; 
              else failed++} 
             END{print pending","running","succeeded","failed}')
    
    # íŒŸ ìƒíƒœ ìˆ˜ì§‘
    pod_stats=$(kubectl get pods -n "$NAMESPACE" --no-headers 2>/dev/null | \
        awk 'BEGIN{running=0; pending=0; succeeded=0; failed=0; unknown=0}
             {if($3=="Running") running++;
              else if($3=="Pending") pending++;
              else if($3=="Succeeded") succeeded++;
              else if($3=="Failed" || $3=="Error") failed++;
              else unknown++}
             END{print running","pending","succeeded","failed","unknown}')
    
    # ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸°
    if [ -f "${DATA_OUTPUT_DIR}/skrueue_large_dataset.db" ]; then
        db_size=$(ls -lh "${DATA_OUTPUT_DIR}/skrueue_large_dataset.db" | awk '{print $5}')
    else
        db_size="N/A"
    fi
    
    # ë¡œê·¸ ì¶œë ¥
    echo "${timestamp} | Jobs: ${job_stats} | Pods: ${pod_stats} | DB: ${db_size}" >> "${LOG_DIR}/monitoring.log"
    
    sleep 300  # 5ë¶„ë§ˆë‹¤ ìˆ˜ì§‘
done
EOF
    
    chmod +x "${LOG_DIR}/monitor.sh"
    
    # ëª¨ë‹ˆí„°ë§ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
    nohup "${LOG_DIR}/monitor.sh" "${LOG_DIR}" "${NAMESPACE}" "${DATA_OUTPUT_DIR}" > /dev/null 2>&1 &
    MONITOR_PID=$!
    echo $MONITOR_PID > "${LOG_DIR}/monitor.pid"
    
    log "ğŸ“ˆ Monitor PID: $MONITOR_PID"
    success "âœ… ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‹œì‘ ì™„ë£Œ"
}

# ë°±ì—… ì‹œìŠ¤í…œ
setup_backup_system() {
    log "ğŸ’¾ ìë™ ë°±ì—… ì‹œìŠ¤í…œ ì„¤ì • ì¤‘..."
    
    # ë°±ì—… ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    cat > "${LOG_DIR}/backup.sh" << EOF
#!/bin/bash
BACKUP_DIR="backups"
DATA_OUTPUT_DIR="$1"
LOG_DIR="$2"

while true; do
    sleep ${BACKUP_INTERVAL}  # 1ì‹œê°„ë§ˆë‹¤ ë°±ì—…
    
    timestamp=\$(date '+%Y%m%d_%H%M%S')
    backup_name="skrueue_backup_\${timestamp}"
    
    echo "\$(date): ë°±ì—… ì‹œì‘ - \${backup_name}" >> "\${LOG_DIR}/backup.log"
    
    # ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…
    if [ -f "\${DATA_OUTPUT_DIR}/skrueue_large_dataset.db" ]; then
        cp "\${DATA_OUTPUT_DIR}/skrueue_large_dataset.db" "\${BACKUP_DIR}/\${backup_name}.db"
        echo "\$(date): DB ë°±ì—… ì™„ë£Œ - \${backup_name}.db" >> "\${LOG_DIR}/backup.log"
    fi
    
    # ë¡œê·¸ ë°±ì—…
    tar -czf "\${BACKUP_DIR}/\${backup_name}_logs.tar.gz" "\${LOG_DIR}/" 2>/dev/null
    echo "\$(date): ë¡œê·¸ ë°±ì—… ì™„ë£Œ - \${backup_name}_logs.tar.gz" >> "\${LOG_DIR}/backup.log"
    
    # ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬ (72ì‹œê°„ ì´ìƒ)
    find "\${BACKUP_DIR}" -name "skrueue_backup_*" -mtime +3 -delete 2>/dev/null
    
done
EOF
    
    chmod +x "${LOG_DIR}/backup.sh"
    
    # ë°±ì—… ì‹œìŠ¤í…œ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
    nohup "${LOG_DIR}/backup.sh" "${DATA_OUTPUT_DIR}" "${LOG_DIR}" > /dev/null 2>&1 &
    BACKUP_PID=$!
    echo $BACKUP_PID > "${LOG_DIR}/backup.pid"
    
    log "ğŸ’¾ Backup System PID: $BACKUP_PID"
    success "âœ… ìë™ ë°±ì—… ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ"
}

# ìƒíƒœ ë¦¬í¬íŠ¸ ìƒì„±
generate_status_report() {
    local hour=$1
    log "ğŸ“Š ${hour}ì‹œê°„ ê²½ê³¼ ìƒíƒœ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘..."
    
    report_file="${LOG_DIR}/status_report_${hour}h.txt"
    
    cat > "$report_file" << EOF
================================
SKRueue ë°ì´í„° ìˆ˜ì§‘ ìƒíƒœ ë¦¬í¬íŠ¸
ì‹œê°„: $(date)
ê²½ê³¼: ${hour}ì‹œê°„ / ${DURATION_HOURS}ì‹œê°„
================================

=== í´ëŸ¬ìŠ¤í„° ìƒíƒœ ===
$(kubectl get nodes --no-headers | wc -l) ë…¸ë“œ í™œì„±

=== ì‘ì—… ìƒíƒœ (ë„¤ì„ìŠ¤í˜ì´ìŠ¤: ${NAMESPACE}) ===
$(kubectl get jobs -n "${NAMESPACE}" --no-headers 2>/dev/null | awk 'BEGIN{pending=0; running=0; succeeded=0; failed=0} {if($2=="0/1") pending++; else if($3=="0") running++; else if($2=="1/1") succeeded++; else failed++} END{printf "ëŒ€ê¸°: %d, ì‹¤í–‰ì¤‘: %d, ì„±ê³µ: %d, ì‹¤íŒ¨: %d\n", pending, running, succeeded, failed}')

ì´ ì‘ì—… ìˆ˜: $(kubectl get jobs -n "${NAMESPACE}" --no-headers 2>/dev/null | wc -l)

=== íŒŸ ìƒíƒœ ===
$(kubectl get pods -n "${NAMESPACE}" --no-headers 2>/dev/null | awk 'BEGIN{running=0; pending=0; succeeded=0; failed=0} {if($3=="Running") running++; else if($3=="Pending") pending++; else if($3=="Succeeded") succeeded++; else if($3=="Failed") failed++} END{printf "ì‹¤í–‰ì¤‘: %d, ëŒ€ê¸°: %d, ì„±ê³µ: %d, ì‹¤íŒ¨: %d\n", running, pending, succeeded, failed}')

=== ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ===
EOF
    
    # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸
    if [ -f "${DATA_OUTPUT_DIR}/skrueue_large_dataset.db" ]; then
        echo "íŒŒì¼ í¬ê¸°: $(ls -lh "${DATA_OUTPUT_DIR}/skrueue_large_dataset.db" | awk '{print $5}')" >> "$report_file"
        
        # ë°ì´í„°ë² ì´ìŠ¤ ë ˆì½”ë“œ ìˆ˜ í™•ì¸
        python3 -c "
import sqlite3
try:
    conn = sqlite3.connect('${DATA_OUTPUT_DIR}/skrueue_large_dataset.db')
    cursor = conn.cursor()
    cursor.execute('SELECT COUNT(*) FROM jobs')
    job_count = cursor.fetchone()[0]
    cursor.execute('SELECT COUNT(*) FROM cluster_states')
    cluster_count = cursor.fetchone()[0]
    print(f'ì‘ì—… ë ˆì½”ë“œ: {job_count:,}ê°œ')
    print(f'í´ëŸ¬ìŠ¤í„° ìƒíƒœ ë ˆì½”ë“œ: {cluster_count:,}ê°œ')
    conn.close()
except Exception as e:
    print(f'DB ì¡°íšŒ ì˜¤ë¥˜: {e}')
" >> "$report_file" 2>/dev/null || echo "DB ë ˆì½”ë“œ ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨" >> "$report_file"
    else
        echo "ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì—†ìŒ" >> "$report_file"
    fi
    
    echo "" >> "$report_file"
    echo "=== í”„ë¡œì„¸ìŠ¤ ìƒíƒœ ===" >> "$report_file"
    
    # í”„ë¡œì„¸ìŠ¤ ìƒíƒœ í™•ì¸
    for pid_file in "${LOG_DIR}"/*.pid; do
        if [ -f "$pid_file" ]; then
            pid=$(cat "$pid_file")
            process_name=$(basename "$pid_file" .pid)
            if ps -p "$pid" > /dev/null; then
                echo "${process_name}: ì‹¤í–‰ì¤‘ (PID: $pid)" >> "$report_file"
            else
                echo "${process_name}: ì¤‘ë‹¨ë¨ (PID: $pid)" >> "$report_file"
            fi
        fi
    done
    
    log "ğŸ“Š ìƒíƒœ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: $report_file"
    
    # ì¤‘ìš” ì •ë³´ ë¡œê·¸ì—ë„ ì¶œë ¥
    cat "$report_file" | head -20 | tee -a "${LOG_DIR}/main.log"
}

# ì •ë¦¬ í•¨ìˆ˜
cleanup_on_exit() {
    log "ğŸ›‘ ì¢…ë£Œ ì‹ í˜¸ ê°ì§€ - ì •ë¦¬ ì‘ì—… ì‹œì‘"
    
    # ëª¨ë“  ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
    for pid_file in "${LOG_DIR}"/*.pid; do
        if [ -f "$pid_file" ]; then
            pid=$(cat "$pid_file")
            process_name=$(basename "$pid_file" .pid)
            if ps -p "$pid" > /dev/null; then
                log "ğŸ›‘ ${process_name} í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì¤‘ (PID: $pid)"
                kill "$pid" 2>/dev/null || true
            fi
        fi
    done
    
    # ìµœì¢… ë°ì´í„° ë‚´ë³´ë‚´ê¸°
    log "ğŸ“Š ìµœì¢… ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì¤‘..."
    if [ -f "${DATA_OUTPUT_DIR}/skrueue_large_dataset.db" ]; then
        python data_collector.py \
            --export-only \
            --db-path "${DATA_OUTPUT_DIR}/skrueue_large_dataset.db" \
            --output "${DATA_OUTPUT_DIR}/final_large_training_data.csv" \
            >> "${LOG_DIR}/final_export.log" 2>&1
        
        if [ -f "${DATA_OUTPUT_DIR}/final_large_training_data.csv" ]; then
            success "âœ… ìµœì¢… CSV ìƒì„± ì™„ë£Œ: ${DATA_OUTPUT_DIR}/final_large_training_data.csv"
        fi
    fi
    
    # ìµœì¢… ìƒíƒœ ë¦¬í¬íŠ¸
    generate_status_report "FINAL"
    
    success "âœ… ì •ë¦¬ ì‘ì—… ì™„ë£Œ"
    exit 0
}

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
main() {
    echo ""
    echo "ğŸš€ SKRueue 24ì‹œê°„ ëŒ€ìš©ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘"
    echo "============================================"
    echo "ğŸ“… ì‹œì‘ ì‹œê°„: $(date)"
    echo "â° ì§€ì† ì‹œê°„: ${DURATION_HOURS}ì‹œê°„"
    echo "ğŸ“Š ìˆ˜ì§‘ ê°„ê²©: ${DATA_COLLECTION_INTERVAL}ì´ˆ"
    echo "ğŸ“‚ ë¡œê·¸ ë””ë ‰í† ë¦¬: ${LOG_DIR}"
    echo "ğŸ’¾ ë°ì´í„° ë””ë ‰í† ë¦¬: ${DATA_OUTPUT_DIR}"
    echo "ğŸ”„ ë°±ì—… ê°„ê²©: ${BACKUP_INTERVAL}ì´ˆ"
    echo "============================================"
    echo ""
    
    # ì‹ í˜¸ ì²˜ë¦¬ê¸° ì„¤ì •
    trap cleanup_on_exit SIGINT SIGTERM
    
    # 1. í™˜ê²½ ì„¤ì •
    setup_environment
    
    # 2. ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
    cleanup_processes
    
    # 3. í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
    check_cluster_status
    
    # 4. ë°ì´í„° ìˆ˜ì§‘ê¸° ì‹œì‘
    start_data_collector
    
    # 5. ì›Œí¬ë¡œë“œ ìƒì„±ê¸° ì‹œì‘
    start_workload_generator
    
    # 6. ì¶”ê°€ ì›Œí¬ë¡œë“œ íŒ¨í„´ ìƒì„±
    generate_additional_workloads
    
    # 7. ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‹œì‘
    start_monitoring
    
    # 8. ë°±ì—… ì‹œìŠ¤í…œ ì„¤ì •
    setup_backup_system
    
    # 9. ë©”ì¸ ë£¨í”„ (ì‹œê°„ë³„ ìƒíƒœ í™•ì¸)
    log "ğŸ”„ ë©”ì¸ ë£¨í”„ ì‹œì‘ - ${DURATION_HOURS}ì‹œê°„ ì‹¤í–‰"
    
    for hour in $(seq 1 ${DURATION_HOURS}); do
        log "â° ${hour}/${DURATION_HOURS}ì‹œê°„ ëŒ€ê¸° ì‹œì‘"
        
        # 1ì‹œê°„ ëŒ€ê¸° (12ë²ˆì˜ 5ë¶„ ì²´í¬)
        for minute_check in {1..12}; do
            sleep 300  # 5ë¶„
            
            # í”„ë¡œì„¸ìŠ¤ ìƒíƒœ í™•ì¸
            if [ -f "${LOG_DIR}/data_collector.pid" ]; then
                pid=$(cat "${LOG_DIR}/data_collector.pid")
                if ! ps -p "$pid" > /dev/null; then
                    error "âŒ ë°ì´í„° ìˆ˜ì§‘ê¸° ì¤‘ë‹¨ ê°ì§€ - ì¬ì‹œì‘ ì‹œë„"
                    start_data_collector
                fi
            fi
            
            if [ -f "${LOG_DIR}/workload_generator.pid" ]; then
                pid=$(cat "${LOG_DIR}/workload_generator.pid")
                if ! ps -p "$pid" > /dev/null; then
                    warning "âš ï¸ ì›Œí¬ë¡œë“œ ìƒì„±ê¸° ì¤‘ë‹¨ ê°ì§€"
                fi
            fi
            
            # 5ë¶„ë§ˆë‹¤ ê°„ë‹¨í•œ ìƒíƒœ ì¶œë ¥
            job_count=$(kubectl get jobs -n "${NAMESPACE}" --no-headers 2>/dev/null | wc -l)
            running_pods=$(kubectl get pods -n "${NAMESPACE}" --field-selector=status.phase=Running --no-headers 2>/dev/null | wc -l)
            log "ğŸ“Š ${hour}ì‹œê°„ ${minute_check}ë²ˆì§¸ ì²´í¬ - ì‘ì—…: ${job_count}ê°œ, ì‹¤í–‰ì¤‘ íŒŸ: ${running_pods}ê°œ"
        done
        
        # ì‹œê°„ë³„ ìƒíƒœ ë¦¬í¬íŠ¸ ìƒì„±
        generate_status_report "$hour"
        
        log "âœ… ${hour}ì‹œê°„ ì™„ë£Œ"
    done
    
    success "ğŸ‰ 24ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!"
    
    # ìµœì¢… ë°ì´í„° ì²˜ë¦¬
    log "ğŸ“Š ìµœì¢… ë°ì´í„° ë‚´ë³´ë‚´ê¸° ë° ì •ë¦¬ ì¤‘..."
    
    # ë°ì´í„° ìˆ˜ì§‘ê¸° ì¢…ë£Œ
    if [ -f "${LOG_DIR}/data_collector.pid" ]; then
        pid=$(cat "${LOG_DIR}/data_collector.pid")
        if ps -p "$pid" > /dev/null; then
            kill "$pid"
            sleep 10
        fi
    fi
    
    # ìµœì¢… CSV ë‚´ë³´ë‚´ê¸°
    if [ -f "${DATA_OUTPUT_DIR}/skrueue_large_dataset.db" ]; then
        log "ğŸ“Š ìµœì¢… ëŒ€ìš©ëŸ‰ í›ˆë ¨ ë°ì´í„° ìƒì„± ì¤‘..."
        python data_collector.py \
            --export-only \
            --db-path "${DATA_OUTPUT_DIR}/skrueue_large_dataset.db" \
            --output "${DATA_OUTPUT_DIR}/large_training_dataset.csv"
        
        if [ -f "${DATA_OUTPUT_DIR}/large_training_dataset.csv" ]; then
            success "âœ… ëŒ€ìš©ëŸ‰ í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!"
            log "ğŸ“ íŒŒì¼ ìœ„ì¹˜: ${DATA_OUTPUT_DIR}/large_training_dataset.csv"
            log "ğŸ“Š íŒŒì¼ í¬ê¸°: $(ls -lh "${DATA_OUTPUT_DIR}/large_training_dataset.csv" | awk '{print $5}')"
            log "ğŸ“ ë ˆì½”ë“œ ìˆ˜: $(wc -l "${DATA_OUTPUT_DIR}/large_training_dataset.csv" | awk '{print $1}')"
        else
            error "âŒ ìµœì¢… CSV ìƒì„± ì‹¤íŒ¨"
        fi
    else
        error "âŒ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ"
    fi
    
    # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
    cleanup_on_exit
}

# ì‚¬ìš©ë²• ì¶œë ¥
usage() {
    echo "ì‚¬ìš©ë²•: $0 [OPTIONS]"
    echo ""
    echo "OPTIONS:"
    echo "  -h, --help           ì´ ë„ì›€ë§ ì¶œë ¥"
    echo "  -d, --duration HOURS ì‹¤í–‰ ì‹œê°„ (ê¸°ë³¸ê°’: 24ì‹œê°„)"
    echo "  -i, --interval SEC   ë°ì´í„° ìˆ˜ì§‘ ê°„ê²© (ê¸°ë³¸ê°’: 10ì´ˆ)"
    echo "  -n, --namespace NS   ëŒ€ìƒ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ (ê¸°ë³¸ê°’: skrueue-test)"
    echo ""
    echo "ì˜ˆì‹œ:"
    echo "  $0                           # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ 24ì‹œê°„ ì‹¤í–‰"
    echo "  $0 -d 12                     # 12ì‹œê°„ë§Œ ì‹¤í–‰"
    echo "  $0 -d 48 -i 5               # 48ì‹œê°„, 5ì´ˆ ê°„ê²©ìœ¼ë¡œ ì‹¤í–‰"
    echo ""
}

# ëª…ë ¹í–‰ ì¸ìˆ˜ ì²˜ë¦¬
while [[ $# -gt 0 ]]; do
    case $1 in
        -h|--help)
            usage
            exit 0
            ;;
        -d|--duration)
            DURATION_HOURS="$2"
            shift 2
            ;;
        -i|--interval)
            DATA_COLLECTION_INTERVAL="$2"
            shift 2
            ;;
        -n|--namespace)
            NAMESPACE="$2"
            shift 2
            ;;
        *)
            error "ì•Œ ìˆ˜ ì—†ëŠ” ì˜µì…˜: $1"
            usage
            exit 1
            ;;
    esac
done

# ì¸ìˆ˜ ê²€ì¦
if ! [[ "$DURATION_HOURS" =~ ^[0-9]+$ ]] || [ "$DURATION_HOURS" -lt 1 ]; then
    error "ìœ íš¨í•˜ì§€ ì•Šì€ ì§€ì† ì‹œê°„: $DURATION_HOURS"
    exit 1
fi

if ! [[ "$DATA_COLLECTION_INTERVAL" =~ ^[0-9]+$ ]] || [ "$DATA_COLLECTION_INTERVAL" -lt 1 ]; then
    error "ìœ íš¨í•˜ì§€ ì•Šì€ ìˆ˜ì§‘ ê°„ê²©: $DATA_COLLECTION_INTERVAL"
    exit 1
fi

# ë©”ì¸ ì‹¤í–‰
main