# skrueue.py (ìˆ˜ì •ëœ ë²„ì „)
# SKRueue: Kueueì™€ RL ì—ì´ì „íŠ¸ í†µí•© ì¸í„°í˜ì´ìŠ¤
# Kubernetes Kueue ìŠ¤ì¼€ì¤„ëŸ¬ì— ê°•í™”í•™ìŠµì„ ì ìš©í•˜ëŠ” í•µì‹¬ ëª¨ë“ˆ

import os
import numpy as np
import pandas as pd
import time
import logging
import json
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from abc import ABC, abstractmethod
import threading
import queue

# Kubernetes ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from kubernetes import client, config
    from kubernetes.client.rest import ApiException
except ImportError:
    print("âŒ kubernetes íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install kubernetes")
    exit(1)

# Gym/Gymnasium í˜¸í™˜ì„± ì²˜ë¦¬
try:
    import gymnasium as gym
    from gymnasium import spaces
    print("âœ… Gymnasium ì‚¬ìš©")
except ImportError:
    try:
        import gym
        from gym import spaces
        print("âœ… êµ¬ Gym ë²„ì „ ì‚¬ìš©")
    except ImportError:
        print("âŒ gym ë˜ëŠ” gymnasium íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install gymnasium")
        exit(1)

# RL ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import torch
    import torch.nn as nn
    from stable_baselines3 import DQN, PPO, A2C
    from stable_baselines3.common.env_checker import check_env
    print("âœ… Stable-Baselines3 ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    print("âŒ í•„ìš”í•œ RL ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤:")
    print("pip install stable-baselines3[extra] torch")
    exit(1)

@dataclass
class JobInfo:
    """ì‘ì—… ì •ë³´"""
    job_id: str
    name: str
    namespace: str
    cpu_request: float
    memory_request: float  # GB
    priority: int
    arrival_time: datetime
    estimated_duration: float  # ë¶„
    job_type: str  # 'spark', 'batch', 'ml' ë“±

@dataclass
class ClusterResource:
    """í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤ ì •ë³´"""
    cpu_total: float
    cpu_available: float
    memory_total: float  # GB
    memory_available: float  # GB
    running_jobs: int
    avg_cpu_utilization: float
    avg_memory_utilization: float
    recent_oom_rate: float

class KueueInterface:
    """Kueue ìŠ¤ì¼€ì¤„ëŸ¬ì™€ì˜ ì¸í„°í˜ì´ìŠ¤ (ê°œì„ ëœ ë²„ì „)"""
    
    def __init__(self, namespaces: List[str] = None):
        self.namespaces = namespaces or ['default']
        self.logger = logging.getLogger('KueueInterface')
        
        # Kubernetes í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            config.load_incluster_config()
            self.logger.info("âœ… In-cluster ì„¤ì • ë¡œë“œ ì„±ê³µ")
        except:
            try:
                config.load_kube_config()
                self.logger.info("âœ… ë¡œì»¬ kubeconfig ë¡œë“œ ì„±ê³µ")
            except Exception as e:
                self.logger.error(f"âŒ Kubernetes ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
                raise
            
        self.k8s_core = client.CoreV1Api()
        self.k8s_batch = client.BatchV1Api()
        self.k8s_custom = client.CustomObjectsApi()
        
    def get_pending_jobs(self) -> List[JobInfo]:
        """ëŒ€ê¸° ì¤‘ì¸ ì‘ì—… ëª©ë¡ ì¡°íšŒ (suspend=true ì‘ì—… í¬í•¨)"""
        pending_jobs = []
        
        for namespace in self.namespaces:
            try:
                jobs = self.k8s_batch.list_namespaced_job(namespace=namespace)
                
                for job in jobs.items:
                    # í•µì‹¬ ìˆ˜ì •: suspend=trueì¸ ì‘ì—…ë§Œ ì„ íƒ
                    if job.spec and job.spec.suspend == True:
                        job_info = self._extract_job_info_from_k8s_job(job, namespace)
                        if job_info:
                            pending_jobs.append(job_info)
                            
            except Exception as e:
                self.logger.error(f"Jobs ì¡°íšŒ ì‹¤íŒ¨ (namespace: {namespace}): {e}")
        
        self.logger.info(f"Found {len(pending_jobs)} suspended jobs")
        return pending_jobs
        
    def get_cluster_resources(self) -> ClusterResource:
        """í˜„ì¬ í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤ ìƒíƒœ ì¡°íšŒ"""
        try:
            nodes = self.k8s_core.list_node()
            
            total_cpu = total_memory = 0.0
            available_cpu = available_memory = 0.0
            
            for node in nodes.items:
                # ìŠ¤ì¼€ì¤„ ê°€ëŠ¥í•œ ë…¸ë“œë§Œ ê³„ì‚°
                if self._is_node_schedulable(node):
                    capacity = node.status.capacity
                    allocatable = node.status.allocatable
                    
                    total_cpu += self._parse_cpu(capacity.get('cpu', '0'))
                    total_memory += self._parse_memory(capacity.get('memory', '0'))
                    available_cpu += self._parse_cpu(allocatable.get('cpu', '0'))
                    available_memory += self._parse_memory(allocatable.get('memory', '0'))
                
            # ì‹¤í–‰ ì¤‘ì¸ ì‘ì—… ìˆ˜ ê³„ì‚°
            running_jobs = self._count_running_jobs()
            
            # ì‚¬ìš©ë¥  ê³„ì‚°
            cpu_used = max(0, total_cpu - available_cpu)
            memory_used = max(0, total_memory - available_memory)
            
            avg_cpu_util = cpu_used / max(total_cpu, 1)
            avg_memory_util = memory_used / max(total_memory, 1)
            
            # ìµœê·¼ OOM ë¹„ìœ¨ ê³„ì‚°
            recent_oom_rate = self._calculate_recent_oom_rate()
            
            return ClusterResource(
                cpu_total=total_cpu,
                cpu_available=available_cpu,
                memory_total=total_memory,
                memory_available=available_memory,
                running_jobs=running_jobs,
                avg_cpu_utilization=avg_cpu_util,
                avg_memory_utilization=avg_memory_util,
                recent_oom_rate=recent_oom_rate
            )
            
        except Exception as e:
            self.logger.error(f"í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return ClusterResource(
                cpu_total=1.0, cpu_available=1.0, memory_total=1.0, 
                memory_available=1.0, running_jobs=0, avg_cpu_utilization=0.0,
                avg_memory_utilization=0.0, recent_oom_rate=0.0
            )
            
    def admit_job(self, job_info: JobInfo) -> bool:
        """ì‘ì—…ì„ ìŠ¹ì¸í•˜ì—¬ ìŠ¤ì¼€ì¤„ë§ ì‹œì‘"""
        try:
            # Kubernetes Jobì˜ suspend ìƒíƒœë¥¼ falseë¡œ ë³€ê²½
            patch_body = {"spec": {"suspend": False}}
            
            result = self.k8s_batch.patch_namespaced_job(
                name=job_info.name,
                namespace=job_info.namespace,
                body=patch_body
            )
            
            self.logger.info(f"ì‘ì—… ìŠ¹ì¸ ì„±ê³µ: {job_info.name}")
            return True
            
        except Exception as e:
            self.logger.error(f"ì‘ì—… ìŠ¹ì¸ ì‹¤íŒ¨ {job_info.name}: {e}")
            
            # Workload íŒ¨ì¹˜ ì‹œë„
            try:
                patch_body = {"spec": {"suspend": False}}
                
                result = self.k8s_custom.patch_namespaced_custom_object(
                    group="kueue.x-k8s.io",
                    version="v1beta1",
                    namespace=job_info.namespace,
                    plural="workloads",
                    name=job_info.name,
                    body=patch_body
                )
                
                self.logger.info(f"Workload ìŠ¹ì¸ ì„±ê³µ: {job_info.name}")
                return True
                
            except Exception as e2:
                self.logger.error(f"Workload ìŠ¹ì¸ë„ ì‹¤íŒ¨ {job_info.name}: {e2}")
                return False
            
    def _extract_job_info_from_k8s_job(self, job, namespace: str) -> Optional[JobInfo]:
        """Kubernetes Jobì—ì„œ JobInfo ì¶”ì¶œ"""
        try:
            metadata = job.metadata
            spec = job.spec
            
            # ê¸°ë³¸ ì •ë³´
            job_id = metadata.uid
            name = metadata.name
            
            # ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
            cpu_request, memory_request, _, _ = self._extract_resources_from_job_spec(spec)
            
            # ìš°ì„ ìˆœìœ„ ì¶”ì¶œ
            priority = self._extract_priority_from_labels(metadata.labels or {})
            
            # ë„ì°© ì‹œê°„
            creation_timestamp = metadata.creation_timestamp
            arrival_time = creation_timestamp if creation_timestamp else datetime.now()
            
            # ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„
            annotations = metadata.annotations or {}
            estimated_duration = float(annotations.get('skrueue.ai/estimated-duration', '30'))
            
            # ì‘ì—… íƒ€ì…
            job_type = self._estimate_job_type_from_labels(metadata.labels or {})
            
            return JobInfo(
                job_id=job_id,
                name=name,
                namespace=namespace,
                cpu_request=cpu_request,
                memory_request=memory_request,
                priority=priority,
                arrival_time=arrival_time,
                estimated_duration=estimated_duration,
                job_type=job_type
            )
            
        except Exception as e:
            self.logger.error(f"K8s Job JobInfo ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def _extract_job_info_from_workload(self, workload: Dict, namespace: str) -> Optional[JobInfo]:
        """Workloadì—ì„œ JobInfo ì¶”ì¶œ"""
        try:
            metadata = workload.get('metadata', {})
            spec = workload.get('spec', {})
            
            # ê¸°ë³¸ ì •ë³´
            job_id = metadata.get('uid', '')
            name = metadata.get('name', '')
            
            # ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­ (ê°„ì†Œí™”)
            pod_sets = spec.get('podSets', [])
            cpu_request = memory_request = 1.0  # ê¸°ë³¸ê°’
            
            if pod_sets:
                pod_set = pod_sets[0]
                template = pod_set.get('template', {}).get('spec', {})
                containers = template.get('containers', [])
                
                if containers:
                    container = containers[0]
                    resources = container.get('resources', {})
                    requests = resources.get('requests', {})
                    
                    cpu_request = self._parse_cpu(requests.get('cpu', '0'))
                    memory_request = self._parse_memory(requests.get('memory', '0'))
            
            # ìš°ì„ ìˆœìœ„ ì¶”ì¶œ
            priority = spec.get('priority', 0)
            
            # ë„ì°© ì‹œê°„
            creation_timestamp = metadata.get('creationTimestamp', '')
            if creation_timestamp:
                arrival_time = datetime.fromisoformat(creation_timestamp.replace('Z', '+00:00'))
            else:
                arrival_time = datetime.now()
            
            # ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„
            annotations = metadata.get('annotations', {})
            estimated_duration = float(annotations.get('skrueue.ai/estimated-duration', '30'))
            
            # ì‘ì—… íƒ€ì…
            job_type = self._estimate_job_type_from_labels(metadata.get('labels', {}))
            
            return JobInfo(
                job_id=job_id,
                name=name,
                namespace=namespace,
                cpu_request=cpu_request,
                memory_request=memory_request,
                priority=priority,
                arrival_time=arrival_time,
                estimated_duration=estimated_duration,
                job_type=job_type
            )
            
        except Exception as e:
            self.logger.error(f"Workload JobInfo ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
            
    def _is_workload_pending(self, workload: Dict) -> bool:
        """Workloadê°€ ëŒ€ê¸° ìƒíƒœì¸ì§€ í™•ì¸"""
        spec = workload.get('spec', {})
        status = workload.get('status', {})
        
        return (spec.get('suspend', False) or not status.get('admitted', False))
        
    def _is_node_schedulable(self, node) -> bool:
        """ë…¸ë“œê°€ ìŠ¤ì¼€ì¤„ë§ ê°€ëŠ¥í•œì§€ í™•ì¸"""
        # Taint í™•ì¸
        if node.spec.taints:
            for taint in node.spec.taints:
                if taint.effect in ['NoSchedule', 'NoExecute']:
                    return False
        
        # Ready ìƒíƒœ í™•ì¸
        if node.status.conditions:
            for condition in node.status.conditions:
                if condition.type == 'Ready' and condition.status == 'True':
                    return True
        
        return False
        
    def _extract_resources_from_job_spec(self, spec) -> Tuple[float, float, float, float]:
        """Job Specì—ì„œ ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ"""
        cpu_request = cpu_limit = 0.0
        memory_request = memory_limit = 0.0
        
        if (spec.template and 
            spec.template.spec and 
            spec.template.spec.containers):
            
            container = spec.template.spec.containers[0]
            
            if container.resources:
                if container.resources.requests:
                    cpu_request = self._parse_cpu(container.resources.requests.get('cpu', '0'))
                    memory_request = self._parse_memory(container.resources.requests.get('memory', '0'))
                    
                if container.resources.limits:
                    cpu_limit = self._parse_cpu(container.resources.limits.get('cpu', '0'))
                    memory_limit = self._parse_memory(container.resources.limits.get('memory', '0'))
                    
        return cpu_request, memory_request, cpu_limit, memory_limit
        
    def _parse_cpu(self, cpu_str: str) -> float:
        """CPU ë¬¸ìì—´ íŒŒì‹±"""
        if not cpu_str or cpu_str == '0':
            return 0.0
        cpu_str = str(cpu_str)
        if cpu_str.endswith('m'):
            return float(cpu_str[:-1]) / 1000.0
        return float(cpu_str)
        
    def _parse_memory(self, memory_str: str) -> float:
        """ë©”ëª¨ë¦¬ ë¬¸ìì—´ì„ GBë¡œ íŒŒì‹±"""
        if not memory_str or memory_str == '0':
            return 0.0
            
        memory_str = str(memory_str)
        units = {'Ki': 1024, 'Mi': 1024**2, 'Gi': 1024**3, 'Ti': 1024**4}
        
        for unit, multiplier in units.items():
            if memory_str.endswith(unit):
                value = float(memory_str[:-len(unit)])
                return (value * multiplier) / (1024**3)
                
        try:
            return float(memory_str) / (1024**3)
        except ValueError:
            return 0.0
        
    def _count_running_jobs(self) -> int:
        """ì‹¤í–‰ ì¤‘ì¸ ì‘ì—… ìˆ˜ ê³„ì‚°"""
        running_count = 0
        
        for namespace in self.namespaces:
            try:
                jobs = self.k8s_batch.list_namespaced_job(namespace=namespace)
                for job in jobs.items:
                    if job.status and job.status.active:
                        running_count += job.status.active
            except Exception:
                pass
                
        return running_count
        
    def _calculate_recent_oom_rate(self) -> float:
        """ìµœê·¼ 1ì‹œê°„ OOM ë¹„ìœ¨ ê³„ì‚°"""
        try:
            cutoff_time = datetime.now(tz=None) - timedelta(hours=1)
            oom_count = 0
            total_events = 0
            
            for namespace in self.namespaces:
                try:
                    events = self.k8s_core.list_namespaced_event(namespace=namespace)
                    
                    for event in events.items:
                        event_time = event.first_timestamp or event.event_time
                        if event_time and event_time.replace(tzinfo=None) > cutoff_time:
                            total_events += 1
                            if ('OOM' in str(event.reason) or 
                                'OutOfMemory' in str(event.message)):
                                oom_count += 1
                except Exception:
                    continue
                            
            return oom_count / max(total_events, 1)
            
        except Exception:
            return 0.0
            
    def _extract_priority_from_labels(self, labels: Dict) -> int:
        """ë ˆì´ë¸”ì—ì„œ ìš°ì„ ìˆœìœ„ ì¶”ì¶œ"""
        try:
            return int(labels.get('priority', 0))
        except (ValueError, TypeError):
            return 0
            
    def _estimate_job_type_from_labels(self, labels: Dict) -> str:
        """ë ˆì´ë¸”ì—ì„œ ì‘ì—… íƒ€ì… ì¶”ì •"""
        for key, value in labels.items():
            key_lower = key.lower()
            value_lower = str(value).lower()
            
            if 'spark' in key_lower or 'spark' in value_lower:
                return 'spark'
            elif any(x in key_lower for x in ['ml', 'machine-learning', 'training']):
                return 'ml'
            elif any(x in key_lower for x in ['etl', 'batch', 'data']):
                return 'batch'
                
        return 'generic'

class SKRueueEnvironment(gym.Env):
    """SKRueue RL í™˜ê²½ (ì›ë˜ ê³ ì°¨ì› ìƒíƒœ ê³µê°„ ìœ ì§€)"""
    
    def __init__(self, kueue_interface: KueueInterface, max_queue_size: int = 10):
        super(SKRueueEnvironment, self).__init__()
        
        self.kueue = kueue_interface
        self.max_queue_size = max_queue_size
        self.logger = logging.getLogger('SKRueueEnvironment')
        
        # ìƒíƒœ ê³µê°„ ì •ì˜ (ì›ë˜ ë²„ì „ ë³µì›)
        # [í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤(4) + íˆìŠ¤í† ë¦¬(3) + ì‘ì—… í(max_queue_size * 6)]
        self.state_dim = 4 + 3 + (max_queue_size * 6)
        self.observation_space = spaces.Box(
            low=0.0, high=1.0, shape=(self.state_dim,), dtype=np.float32
        )
        
        self.logger.info(f"ìƒíƒœ ê³µê°„ ì°¨ì›: {self.state_dim} (í í¬ê¸°: {max_queue_size})")
        
        # í–‰ë™ ê³µê°„ ì •ì˜ (ëŒ€ê¸°ì—´ ì¸ë±ìŠ¤ + wait ì•¡ì…˜)
        self.action_space = spaces.Discrete(max_queue_size + 1)
        
        self.logger.info(f"í–‰ë™ ê³µê°„ í¬ê¸°: {max_queue_size + 1} (ì‘ì—… ì„ íƒ {max_queue_size}ê°œ + ëŒ€ê¸° 1ê°œ)")
        
        # í™˜ê²½ ìƒíƒœ
        self.current_jobs: List[JobInfo] = []
        self.current_resources: Optional[ClusterResource] = None
        self.step_count = 0
        self.episode_start_time = time.time()
        
        # ì„±ëŠ¥ ì§€í‘œ
        self.completed_jobs_count = 0
        self.failed_jobs_count = 0
        self.total_wait_time = 0.0
        
        # ë³´ìƒ í•¨ìˆ˜ ê°€ì¤‘ì¹˜
        self.reward_weights = {
            'throughput': 0.4,
            'utilization': 0.3,
            'wait_penalty': 0.2,
            'failure_penalty': 0.1
        }
        
    def reset(self, seed=None, options=None) -> Tuple[np.ndarray, Dict]:
        """í™˜ê²½ ì´ˆê¸°í™” (Gymnasium í‘œì¤€ ì¤€ìˆ˜)"""
        if seed is not None:
            np.random.seed(seed)
            
        self.step_count = 0
        self.episode_start_time = time.time()
        self.completed_jobs_count = 0
        self.failed_jobs_count = 0
        self.total_wait_time = 0.0
        
        # í˜„ì¬ ìƒíƒœ ê°±ì‹ 
        self._update_state()
        
        obs = self._get_observation()
        info = self._get_info()
        
        # Gymnasium í‘œì¤€: í•­ìƒ (observation, info) íŠœí”Œ ë°˜í™˜
        return obs, info
        
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """í™˜ê²½ ìŠ¤í… ì‹¤í–‰"""
        # ì´ì „ ìƒíƒœ ì €ì¥
        prev_queue_length = len(self.current_jobs)
        prev_utilization = self._get_cluster_utilization()
        
        # ì•¡ì…˜ ì‹¤í–‰
        action_success = self._execute_action(action)
        
        # ìƒíƒœ ê°±ì‹ 
        self._update_state()
        
        # ë³´ìƒ ê³„ì‚°
        reward = self._calculate_reward(prev_queue_length, prev_utilization, action_success)
        
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì¡°ê±´
        terminated = self._is_episode_terminated()
        truncated = self._is_episode_truncated()
        
        # ì •ë³´ êµ¬ì„±
        info = self._get_info()
        info.update({
            'action_success': action_success,
            'prev_queue_length': prev_queue_length
        })
        
        self.step_count += 1
        
        obs = self._get_observation()
        
        # Gymnasium ìŠ¤íƒ€ì¼ ë°˜í™˜ (5ê°œ ê°’)
        return obs, reward, terminated, truncated, info
        
    def _update_state(self):
        """í˜„ì¬ í´ëŸ¬ìŠ¤í„° ë° ì‘ì—… ìƒíƒœ ê°±ì‹ """
        try:
            self.current_jobs = self.kueue.get_pending_jobs()
            self.current_resources = self.kueue.get_cluster_resources()
        except Exception as e:
            self.logger.error(f"ìƒíƒœ ê°±ì‹  ì‹¤íŒ¨: {e}")
            
    def _get_observation(self) -> np.ndarray:
        """í˜„ì¬ ìƒíƒœë¥¼ observationìœ¼ë¡œ ë³€í™˜"""
            # ìƒíƒœ ê³µê°„ ì •ì˜ (ì›ë˜ ë²„ì „ ë³µì›)
            # [í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤(4) + íˆìŠ¤í† ë¦¬(3) + ì‘ì—… í(max_queue_size * 6)]
                # ì°¨ì› 0-3: í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤ ì •ë³´
                    # obs[0]: CPU ê°€ìš©ë¥  (available/total)
                    # obs[1]: ë©”ëª¨ë¦¬ ê°€ìš©ë¥  (available/total)
                    # obs[2]: í‰ê·  CPU ì‚¬ìš©ë¥ 
                    # obs[3]: í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
                # ì°¨ì› 4-6: í´ëŸ¬ìŠ¤í„° íˆìŠ¤í† ë¦¬
                    # obs[4]: ì‹¤í–‰ ì¤‘ì¸ ì‘ì—… ìˆ˜ (ì •ê·œí™”)
                    # obs[5]: CPU ì‚¬ìš©ë¥  (ì¤‘ë³µ, íˆìŠ¤í† ë¦¬ìš©)
                    # obs[6]: ìµœê·¼ OOM ë°œìƒë¥ 
                # ì°¨ì› 7-66: ì‘ì—… í ì •ë³´ (10ê°œ ì‘ì—… Ã— 6ì°¨ì›)
                    # ê° ì‘ì—…ë‹¹ 6ì°¨ì›:
                        # [i*6 + 0]: CPU ìš”ì²­ëŸ‰ (ì •ê·œí™”)
                        # [i*6 + 1]: ë©”ëª¨ë¦¬ ìš”ì²­ëŸ‰ (ì •ê·œí™”)
                        # [i*6 + 2]: ìš°ì„ ìˆœìœ„ (ì •ê·œí™”)
                        # [i*6 + 3]: ëŒ€ê¸° ì‹œê°„ (ì •ê·œí™”)
                        # [i*6 + 4]: ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ (ì •ê·œí™”)
                        # [i*6 + 5]: ì‘ì—… íƒ€ì… (ì¸ì½”ë”©)
        obs = np.zeros(self.state_dim, dtype=np.float32)
        
        if self.current_resources:
            # í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤ ì •ë³´ (ì •ê·œí™”)
            obs[0] = self.current_resources.cpu_available / max(self.current_resources.cpu_total, 1)
            obs[1] = self.current_resources.memory_available / max(self.current_resources.memory_total, 1)
            obs[2] = self.current_resources.avg_cpu_utilization
            obs[3] = self.current_resources.avg_memory_utilization
            
            # íˆìŠ¤í† ë¦¬ ì •ë³´
            obs[4] = min(self.current_resources.running_jobs / 100.0, 1.0)  # ì •ê·œí™”
            obs[5] = self.current_resources.avg_cpu_utilization
            obs[6] = self.current_resources.recent_oom_rate
            
        # ì‘ì—… í ì •ë³´ (ê° ì‘ì—…ë‹¹ 6ì°¨ì›)
        queue_start_idx = 7
        for i, job in enumerate(self.current_jobs[:self.max_queue_size]):
            base_idx = queue_start_idx + (i * 6)
            
            # ì‘ì—… ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­ (ì •ê·œí™”)
            max_cpu = self.current_resources.cpu_total if self.current_resources else 100
            max_memory = self.current_resources.memory_total if self.current_resources else 100
            
            obs[base_idx] = min(job.cpu_request / max_cpu, 1.0)
            obs[base_idx + 1] = min(job.memory_request / max_memory, 1.0)
            obs[base_idx + 2] = min(job.priority / 10.0, 1.0)  # ìš°ì„ ìˆœìœ„ ì •ê·œí™”
            
            # ëŒ€ê¸° ì‹œê°„ (ë¶„ ë‹¨ìœ„, ì •ê·œí™”)
            wait_time = (datetime.now() - job.arrival_time).total_seconds() / 60.0
            obs[base_idx + 3] = min(wait_time / 60.0, 1.0)  # ìµœëŒ€ 1ì‹œê°„ìœ¼ë¡œ ì •ê·œí™”
            
            # ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ (ì •ê·œí™”)
            obs[base_idx + 4] = min(job.estimated_duration / 120.0, 1.0)  # ìµœëŒ€ 2ì‹œê°„ìœ¼ë¡œ ì •ê·œí™”
            
            # ì‘ì—… íƒ€ì… (ì›-í•« ì¸ì½”ë”© ë‹¨ìˆœí™”)
            job_type_map = {'spark': 0.8, 'ml': 0.6, 'batch': 0.4, 'generic': 0.2}
            obs[base_idx + 5] = job_type_map.get(job.job_type, 0.2)
            
        return obs
        
    def _execute_action(self, action: int) -> bool:
        """ì„ íƒëœ ì•¡ì…˜ ì‹¤í–‰"""
        if action >= len(self.current_jobs):
            # Wait ì•¡ì…˜ ë˜ëŠ” ì˜ëª»ëœ ì•¡ì…˜
            return True
            
        # ì„ íƒëœ ì‘ì—… ìŠ¹ì¸
        selected_job = self.current_jobs[action]
        success = self.kueue.admit_job(selected_job)
        
        if success:
            self.logger.info(f"ì‘ì—… ìŠ¹ì¸: {selected_job.name}")
            
        return success
        
    def _calculate_reward(self, prev_queue_length: int, prev_utilization: float, action_success: bool) -> float:
        """ë³´ìƒ í•¨ìˆ˜ ê³„ì‚° (ë‹¨ìˆœí™”)"""
        reward = 0.0
        
        if not self.current_resources:
            return -1.0  # ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨ í˜ë„í‹°
            
        # 1. ì²˜ë¦¬ëŸ‰ ë³´ìƒ (í ê¸¸ì´ ê°ì†Œ)
        queue_reduction = max(0, prev_queue_length - len(self.current_jobs))
        throughput_reward = queue_reduction * self.reward_weights['throughput']
        
        # 2. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ë³´ìƒ
        current_utilization = self._get_cluster_utilization()
        utilization_reward = current_utilization * self.reward_weights['utilization']
        
        # 3. ëŒ€ê¸°ì‹œê°„ í˜ë„í‹°
        if self.current_jobs:
            avg_wait_time = np.mean([
                (datetime.now() - job.arrival_time).total_seconds() / 60.0 
                for job in self.current_jobs
            ])
            wait_penalty = min(avg_wait_time / 60.0, 1.0) * self.reward_weights['wait_penalty']
        else:
            wait_penalty = 0.0
            
        # 4. ì‹¤íŒ¨ í˜ë„í‹°
        failure_penalty = 0.0
        if not action_success:
            failure_penalty = self.reward_weights['failure_penalty']
            
        # ìµœì¢… ë³´ìƒ
        reward = throughput_reward + utilization_reward - wait_penalty - failure_penalty
        
        return reward
        
    def _get_cluster_utilization(self) -> float:
        """í´ëŸ¬ìŠ¤í„° ì „ì²´ ì‚¬ìš©ë¥  ê³„ì‚°"""
        if not self.current_resources:
            return 0.0
            
        cpu_util = self.current_resources.avg_cpu_utilization
        memory_util = self.current_resources.avg_memory_utilization
        
        return (cpu_util + memory_util) / 2.0
        
    def _is_episode_terminated(self) -> bool:
        """ì—í”¼ì†Œë“œ ì •ìƒ ì¢…ë£Œ ì¡°ê±´"""
        # íê°€ ë¹„ê³  ì¶©ë¶„í•œ ì‹œê°„ì´ ì§€ë‚¬ì„ ë•Œ
        return (len(self.current_jobs) == 0 and self.step_count > 20)
        
    def _is_episode_truncated(self) -> bool:
        """ì—í”¼ì†Œë“œ ê°•ì œ ì¢…ë£Œ ì¡°ê±´"""
        # ìµœëŒ€ ìŠ¤í… ìˆ˜ ë„ë‹¬ ë˜ëŠ” ì‹œê°„ ì´ˆê³¼
        max_steps = 200
        max_time = 600  # 10ë¶„
        
        return (self.step_count >= max_steps or 
                time.time() - self.episode_start_time > max_time or
                self.current_resources is None)
                
    def _get_info(self) -> Dict:
        """ì •ë³´ ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
        return {
            'step_count': self.step_count,
            'queue_length': len(self.current_jobs),
            'cluster_utilization': self._get_cluster_utilization(),
            'completed_jobs': self.completed_jobs_count,
            'failed_jobs': self.failed_jobs_count
        }

class RLAgent:
    """SKRueue RL ì—ì´ì „íŠ¸ (ê°œì„ ëœ ë²„ì „)"""
    
    def __init__(self, env: SKRueueEnvironment, algorithm: str = 'DQN'):
        self.env = env
        self.algorithm = algorithm
        self.logger = logging.getLogger('RLAgent')
        self.model = None
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_model()
        
    def _initialize_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™” (ê³ ì°¨ì› ìƒíƒœ ê³µê°„ì— ìµœì í™”)"""
        try:
            # ìƒíƒœ ê³µê°„ í¬ê¸°ì— ë”°ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •
            state_dim = self.env.observation_space.shape[0]
            
            if self.algorithm == 'DQN':
                # ê³ ì°¨ì› ìƒíƒœì— ë§ëŠ” DQN ì„¤ì •
                self.model = DQN(
                    'MlpPolicy', 
                    self.env, 
                    learning_rate=1e-4,
                    buffer_size=50000,  # ë” í° ë²„í¼
                    learning_starts=1000,
                    target_update_interval=500,
                    train_freq=4,
                    exploration_initial_eps=1.0,
                    exploration_final_eps=0.05,
                    exploration_fraction=0.5,  # ë” ê¸´ íƒí—˜ ê¸°ê°„
                    policy_kwargs=dict(net_arch=[256, 256, 128]),  # ë” í° ë„¤íŠ¸ì›Œí¬
                    verbose=1
                )
            elif self.algorithm == 'PPO':
                # ê³ ì°¨ì› ìƒíƒœì— ë§ëŠ” PPO ì„¤ì •
                self.model = PPO(
                    'MlpPolicy', 
                    self.env, 
                    learning_rate=3e-4,
                    n_steps=1024,  # ë” ë§ì€ ìŠ¤í…
                    batch_size=64,
                    n_epochs=10,
                    gamma=0.99,
                    gae_lambda=0.95,
                    clip_range=0.2,
                    policy_kwargs=dict(net_arch=[256, 256, 128]),  # ë” í° ë„¤íŠ¸ì›Œí¬
                    verbose=1
                )
            elif self.algorithm == 'A2C':
                # ê³ ì°¨ì› ìƒíƒœì— ë§ëŠ” A2C ì„¤ì •
                self.model = A2C(
                    'MlpPolicy', 
                    self.env, 
                    learning_rate=7e-4,
                    n_steps=8,  # ë” ë§ì€ ìŠ¤í…
                    gamma=0.99,
                    gae_lambda=1.0,
                    ent_coef=0.01,
                    vf_coef=0.25,
                    policy_kwargs=dict(net_arch=[256, 256]),
                    verbose=1
                )
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì•Œê³ ë¦¬ì¦˜: {self.algorithm}")
                
            self.logger.info(f"{self.algorithm} ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (ìƒíƒœ ì°¨ì›: {state_dim})")
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
            
    def train(self, total_timesteps: int = 10000):
        """ëª¨ë¸ í›ˆë ¨"""
        self.logger.info(f"{self.algorithm} í›ˆë ¨ ì‹œì‘ (ì´ {total_timesteps} ìŠ¤í…)")
        
        try:
            self.model.learn(total_timesteps=total_timesteps)
            self.logger.info("í›ˆë ¨ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"í›ˆë ¨ ì‹¤íŒ¨: {e}")
            raise
            
    def save_model(self, path: str):
        """ëª¨ë¸ ì €ì¥"""
        try:
            os.makedirs(os.path.dirname(path), exist_ok=True)
            self.model.save(path)
            self.logger.info(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {path}")
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            
    def load_model(self, path: str):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            if self.algorithm == 'DQN':
                self.model = DQN.load(path, env=self.env)
            elif self.algorithm == 'PPO':
                self.model = PPO.load(path, env=self.env)
            elif self.algorithm == 'A2C':
                self.model = A2C.load(path, env=self.env)
                
            self.logger.info(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {path}")
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
    def predict(self, observation: np.ndarray) -> int:
        """ì•¡ì…˜ ì˜ˆì¸¡"""
        try:
            action, _ = self.model.predict(observation, deterministic=True)
            return int(action)
        except Exception as e:
            self.logger.error(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return 0  # ê¸°ë³¸ ì•¡ì…˜
        
    def evaluate(self, num_episodes: int = 5) -> Dict[str, float]:
        """ëª¨ë¸ í‰ê°€"""
        total_rewards = []
        episode_lengths = []
        success_rates = []
        
        for episode in range(num_episodes):
            obs, info = self.env.reset()  # íŠœí”Œ ì–¸íŒ¨í‚¹ ìˆ˜ì •
            total_reward = 0
            steps = 0
            successful_actions = 0
            total_actions = 0
            
            terminated = truncated = False
            while not (terminated or truncated):
                action = self.predict(obs)
                obs, reward, terminated, truncated, info = self.env.step(action)
                
                total_reward += reward
                steps += 1
                total_actions += 1
                
                if info.get('action_success', False):
                    successful_actions += 1
                    
            total_rewards.append(total_reward)
            episode_lengths.append(steps)
            success_rates.append(successful_actions / max(total_actions, 1))
            
        return {
            'mean_reward': np.mean(total_rewards),
            'std_reward': np.std(total_rewards),
            'mean_episode_length': np.mean(episode_lengths),
            'mean_success_rate': np.mean(success_rates)
        }

def create_skrueue_environment(namespaces: List[str] = None, max_queue_size: int = 10) -> SKRueueEnvironment:
    """SKRueue í™˜ê²½ ìƒì„± í—¬í¼ í•¨ìˆ˜"""
    if namespaces is None:
        namespaces = ['skrueue-test']
        
    try:
        kueue_interface = KueueInterface(namespaces)
        env = SKRueueEnvironment(kueue_interface, max_queue_size=max_queue_size)
        
        # í™˜ê²½ ê²€ì¦
        check_env(env)
        
        print(f"âœ… SKRueue í™˜ê²½ ìƒì„± ì™„ë£Œ:")
        print(f"   - ìƒíƒœ ê³µê°„: {env.state_dim}ì°¨ì›")
        print(f"   - í–‰ë™ ê³µê°„: {env.action_space.n}ê°œ ì•¡ì…˜")
        print(f"   - ìµœëŒ€ í í¬ê¸°: {max_queue_size}")
        
        return env
    except Exception as e:
        print(f"âŒ í™˜ê²½ ìƒì„± ì‹¤íŒ¨: {e}")
        raise

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='SKRueue RL ì‹œìŠ¤í…œ')
    parser.add_argument('--mode', choices=['train', 'eval', 'inference', 'test'], required=True)
    parser.add_argument('--algorithm', choices=['DQN', 'PPO', 'A2C'], default='DQN')
    parser.add_argument('--model-path', type=str, help='ëª¨ë¸ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--timesteps', type=int, default=20000, help='í›ˆë ¨ ìŠ¤í… ìˆ˜ (ê³ ì°¨ì› í™˜ê²½ ê³ ë ¤)')
    parser.add_argument('--namespaces', nargs='*', default=['skrueue-test'], help='ëª¨ë‹ˆí„°ë§ ë„¤ì„ìŠ¤í˜ì´ìŠ¤')
    parser.add_argument('--episodes', type=int, default=5, help='í‰ê°€ ì—í”¼ì†Œë“œ ìˆ˜')
    parser.add_argument('--max-queue-size', type=int, default=10, help='ìµœëŒ€ í í¬ê¸° (ìƒíƒœ ê³µê°„ ì°¨ì›ì— ì˜í–¥)')
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    try:
        # í™˜ê²½ ìƒì„±
        print(f"ğŸ”§ SKRueue í™˜ê²½ ìƒì„± ì¤‘... (ë„¤ì„ìŠ¤í˜ì´ìŠ¤: {args.namespaces}, í í¬ê¸°: {args.max_queue_size})")
        env = create_skrueue_environment(args.namespaces, args.max_queue_size)
        print("âœ… í™˜ê²½ ìƒì„± ì™„ë£Œ")
        
        # ì—ì´ì „íŠ¸ ìƒì„±
        print(f"ğŸ¤– {args.algorithm} ì—ì´ì „íŠ¸ ìƒì„± ì¤‘...")
        agent = RLAgent(env, args.algorithm)
        print("âœ… ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ")
        
        if args.mode == 'test':
            # í™˜ê²½ í…ŒìŠ¤íŠ¸
            print("ğŸ§ª í™˜ê²½ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
            obs, info = env.reset()  # íŠœí”Œ ì–¸íŒ¨í‚¹ ìˆ˜ì •
            print(f"ê´€ì°° ê³µê°„: {env.observation_space} ({env.state_dim}ì°¨ì›)")
            print(f"í–‰ë™ ê³µê°„: {env.action_space} ({env.action_space.n}ê°œ ì•¡ì…˜)")
            print(f"ì´ˆê¸° ê´€ì°° (ì²˜ìŒ 10ì°¨ì›): {obs[:10]}")
            print(f"ì´ˆê¸° ì •ë³´: {info}")
            print(f"ìƒíƒœ êµ¬ì¡°:")
            print(f"  - í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤: ì°¨ì› 0-3")
            print(f"  - íˆìŠ¤í† ë¦¬: ì°¨ì› 4-6") 
            print(f"  - ì‘ì—… í: ì°¨ì› 7-{env.state_dim-1} (ì‘ì—…ë‹¹ 6ì°¨ì›)")
            
            for i in range(3):
                action = env.action_space.sample()
                obs, reward, terminated, truncated, info = env.step(action)
                print(f"Step {i+1}: action={action}, reward={reward:.3f}, info={info}")
                
                if terminated or truncated:
                    obs, info = env.reset()  # íŠœí”Œ ì–¸íŒ¨í‚¹ ìˆ˜ì •
                    
            print("âœ… í™˜ê²½ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            
        elif args.mode == 'train':
            # í›ˆë ¨ ëª¨ë“œ
            print(f"ğŸ¯ {args.algorithm} í›ˆë ¨ ì‹œì‘...")
            agent.train(args.timesteps)
            
            if args.model_path:
                agent.save_model(args.model_path)
                print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {args.model_path}")
                
        elif args.mode == 'eval':
            # í‰ê°€ ëª¨ë“œ
            if args.model_path:
                print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì¤‘: {args.model_path}")
                agent.load_model(args.model_path)
            
            print(f"ğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘... ({args.episodes} ì—í”¼ì†Œë“œ)")
            results = agent.evaluate(args.episodes)
            
            print("ğŸ“ˆ í‰ê°€ ê²°ê³¼:")
            for key, value in results.items():
                print(f"  {key}: {value:.4f}")
                
        elif args.mode == 'inference':
            # ì¶”ë¡  ëª¨ë“œ (ì‹¤ì œ ìš´ìš©)
            if not args.model_path:
                raise ValueError("ì¶”ë¡  ëª¨ë“œì—ì„œëŠ” ëª¨ë¸ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤")
                
            print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì¤‘: {args.model_path}")
            agent.load_model(args.model_path)
            
            print("ğŸš€ SKRueue ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘...")
            obs, info = env.reset()  # íŠœí”Œ ì–¸íŒ¨í‚¹ ìˆ˜ì •
            
            try:
                step_count = 0
                while True:
                    action = agent.predict(obs)
                    obs, reward, terminated, truncated, info = env.step(action)
                    
                    step_count += 1
                    print(f"Step {step_count}: action={action}, reward={reward:.3f}, "
                          f"queue={info['queue_length']}, util={info['cluster_utilization']:.3f}")
                    
                    if terminated or truncated:
                        print("ì—í”¼ì†Œë“œ ì¢…ë£Œ, í™˜ê²½ ë¦¬ì…‹")
                        obs, info = env.reset()  # íŠœí”Œ ì–¸íŒ¨í‚¹ ìˆ˜ì •
                        step_count = 0
                        
                    time.sleep(5)  # 5ì´ˆ ê°„ê²©ìœ¼ë¡œ ì‹¤í–‰
                    
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
                
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()